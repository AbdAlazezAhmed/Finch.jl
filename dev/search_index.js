var documenterSearchIndex = {"docs":
[{"location":"directory_structure/#Directory-Structure","page":"Directory Structure","title":"Directory Structure","text":"","category":"section"},{"location":"directory_structure/","page":"Directory Structure","title":"Directory Structure","text":"Here's a little roadmap to the Finch codebase! Please file an issue if this is not up to date.","category":"page"},{"location":"directory_structure/","page":"Directory Structure","title":"Directory Structure","text":".\n├── apps                       # Example applications implemented in Finch!\n│   ├── graphs.jl              # Graph Algorithms: Pagerank, Bellman-Ford, etc...\n│   ├── linalg.jl              # Linear Algebra: Sparse-Sparse Matmul, etc...\n│   └── ...\n├── benchmark                  # benchmarks for internal use\n│   ├── runbenchmarks.jl       # run benchmarks\n│   ├── runjudge.jl            # run benchmarks on current branch and compare with main\n│   └── ...                 \n├── docs                       # documentation\n│   ├── [build]                # rendered docs website\n│   ├── src                    # docs website source\n│   ├── fix.jl                 # fix docstrings\n│   ├── make.jl                # build documentation locally\n│   └── ...                 \n├── embed                      # wrappers for embedding Finch in C\n├── ext                        # conditionally-loaded code for interaction with other packages (e.g. SparseArrays)\n├── src                        # Source files\n│   ├── base                   # Implementations of base functions (e.g. map, reduce, etc.)\n│   ├── fileio                 # File IO function definitions\n│   ├── FinchNotation          # SubModule containing the Finch IR\n│   │   ├── nodes.jl           # defines the Finch IR\n│   │   ├── syntax.jl          # defines the @finch frontend syntax\n│   │   └── ...\n│   ├── looplets               # this is where all the Looplets live\n│   ├── symbolic               # term rewriting systems for program and bounds\n│   ├── tensors                # built-in Finch tensor definitions\n│   │   ├── levels             # all of the levels\n│   │   ├── fibers.jl          # fibers combine levels to form tensors\n│   │   ├── scalars.jl         # a nice scalar type\n│   │   └── masks.jl           # mask tensors (e.g. upper-triangular mask)\n│   ├── transformations        # global program transformations\n│   │   ├── scopes.jl          # gives unique names to indices\n│   │   ├── lifetimes.jl       # adds freeze and thaw\n│   │   ├── dimensionalize.jl  # computes extents for loops and declarations\n│   │   ├── concordize.jl      # adds loops to ensure all accesses are concordant\n│   │   └── wrapperize.jl      # converts index expressions to array wrappers\n│   ├──  execute.jl            # global compiler calls\n│   ├──  lower.jl              # inner compiler definition\n│   ├──  semantics.jl          # finch array interface functions\n│   ├──  traits.jl             # functions and types to reason about appropriate outputs\n│   ├──  util.jl               # shims and julia codegen utils (Dead code elimination, etc...)\n│   └── ...\n├── test                       # tests\n│   ├──  embed                 # tests for the C embedding. Optional build before runtests.jl\n│   ├──  reference32           # reference output for 32-bit systems\n│   ├──  reference64           # reference output for 64-bit systems\n│   ├──  runtests.jl           # runs the test suite. (pass -h for options and more info!)\n│   └── ...\n├── Project.toml               # julia-readable listing of project dependencies\n├── [Manifest.toml]            # local listing of installed dependencies (don't commit this)\n├── LICENSE\n└── README.md","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"CurrentModule = Finch","category":"page"},{"location":"algebra/#Custom-Functions","page":"Custom Functions","title":"Custom Functions","text":"","category":"section"},{"location":"algebra/#User-Functions","page":"Custom Functions","title":"User Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch supports arbitrary Julia Base functions over isbits types.  You can also use your own functions and use them in Finch! Just remember to define any special algebraic properties of your functions so that Finch can optimize them better. You must declare the properties of your functions before you call any Finch functions on them.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch only supports incrementing assignments to arrays such as += or *=. If you would like to increment A[i...] by the value of ex with a custom reduction operator op, you may use the following syntax: A[i...] <<op>>= ex.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Consider the greatest common divisor function gcd. This function is associative and commutative, and the greatest common divisor of 1 and anything else is 1, so 1 is an annihilator.  We declare these properties by overloading trait functions on Finch's default algebra as follows:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch.isassociative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::Finch.DefaultAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Then, the following code will only call gcd when neither u[i] nor v[i] are 1 (just once!).","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"u = Fiber!(SparseList(Element(1)), [3, 1, 6, 1, 9, 1, 4, 1, 8, 1])\nv = Fiber!(SparseList(Element(1)), [1, 2, 3, 1, 1, 1, 1, 4, 1, 1])\nw = Fiber!(SparseList(Element(1)))\n\n@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end)","category":"page"},{"location":"algebra/#A-Few-Convenient-Functions","page":"Custom Functions","title":"A Few Convenient Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"For your convenience, Finch defines a few useful functions that help express common array operations inside Finch:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"choose\nminby\nmaxby","category":"page"},{"location":"algebra/#Finch.choose","page":"Custom Functions","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = Fiber!(SparseList(Element(0.0)), [0, 1.1, 0, 4.4, 0])\nSparseList (0.0) [1:5]\n├─[2]: 1.1\n├─[4]: 4.4\n\njulia> x = Scalar(0.0); @finch for i=_; x[] <<choose(0.0)>>= a[i] end;\n\njulia> x[]\n1.1\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.minby","page":"Custom Functions","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch for i=_; x[] <<minby>>= a[i] => i end;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.maxby","page":"Custom Functions","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch for i=_; x[] <<maxby>>= a[i] => i end;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Properties","page":"Custom Functions","title":"Properties","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"The full list of properties recognized by Finch is as follows (use these to declare the properties of your own functions):","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"isassociative\niscommutative\nisdistributive\nisidempotent\nisidentity\nisannihilator\nisinverse\nisinvolution","category":"page"},{"location":"algebra/#Finch.isassociative","page":"Custom Functions","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.iscommutative","page":"Custom Functions","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isdistributive","page":"Custom Functions","title":"Finch.isdistributive","text":"isdistributive(algebra, f, g)\n\nReturn true when f(a, g(b, c)) = g(f(a, b), f(a, c)) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isidempotent","page":"Custom Functions","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isidentity","page":"Custom Functions","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isannihilator","page":"Custom Functions","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isinverse","page":"Custom Functions","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isinvolution","page":"Custom Functions","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch-Kernel-Caching","page":"Custom Functions","title":"Finch Kernel Caching","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch code is cached when you first run it. Thus, if you run a Finch function once, then make changes to the Finch compiler (like defining new properties), the cached code will be used and the changes will not be reflected.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"It's best to design your code so that modifications to the Finch compiler occur before any Finch functions are called. However, if you really need to modify a precompiled Finch kernel, you can call Finch.refresh() to invalidate the code cache.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"refresh","category":"page"},{"location":"algebra/#Finch.refresh","page":"Custom Functions","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#(Advanced)-On-World-Age-and-Generated-Functions","page":"Custom Functions","title":"(Advanced) On World-Age and Generated Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Julia uses a \"world age\" to describe the set of defined functions at a point in time. Generated functions run in the same world age in which they were defined, so they can't call functions defined after the generated function. This means that if Finch used normal generated functions, users can't define their own functions without first redefining all of Finch's generated functions.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch uses special generators that run in the current world age, but do not update with subsequent compiler function invalidations. If two packages modify the behavior of Finch in different ways, and call those Finch functions during precompilation, the resulting behavior is undefined.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"There are several packages that take similar, but different, approaches to allow user participation in staged Julia programming (not to mention Base eval or @generated): StagedFunctions.jl, GeneralizedGenerated.jl, RuntimeGeneratedFunctions.jl, or Zygote.jl.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Our approach is most similar to that of StagedFunctions.jl or Zygote.jl. We chose our approach to be the simple and flexible while keeping the kernel call overhead low.","category":"page"},{"location":"algebra/#(Advanced)-Separate-Algebras","page":"Custom Functions","title":"(Advanced) Separate Algebras","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"If you want to define non-standard properties or custom rewrite rules for some functions in a separate context, you can represent these changes with your own algebra type.  We express this by subtyping AbstractAlgebra and defining properties as follows:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"struct MyAlgebra <: AbstractAlgebra end\n\nFinch.virtualize(ex, ::Type{MyAlgebra}, ctx) = MyAlgebra()\n\nFinch.isassociative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::MyAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"We pass the algebra to Finch as an optional first argument:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"@finch MyAlgebra() (w .= 1; for i=_; w[i] = gcd(u[i], v[i]) end)","category":"page"},{"location":"algebra/#Rewriting","page":"Custom Functions","title":"Rewriting","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Define custom rewrite rules by overloading the get_program_rules function on your algebra.  Unless you want to write the full rule set from scratch, be sure to append your new rules to the old rules, which can be obtained by calling get_program_rules with another algebra. Rules can be specified directly on Finch IR using RewriteTools.jl.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"get_program_rules\nget_bounds_rules","category":"page"},{"location":"algebra/#Finch.get_program_rules","page":"Custom Functions","title":"Finch.get_program_rules","text":"get_program_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.get_bounds_rules","page":"Custom Functions","title":"Finch.get_bounds_rules","text":"get_bounds_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"function"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"CurrentModule = Finch","category":"page"},{"location":"performance/#Performance-Tips-for-Finch","page":"Performance Tips","title":"Performance Tips for Finch","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"It's easy to ask Finch to run the same operation in different ways. However, different approaches have different performance. The right approach really depends on your particular situation. Here's a collection of general approaches that help Finch generate faster code in most cases.","category":"page"},{"location":"performance/#Concordant-Iteration","page":"Performance Tips","title":"Concordant Iteration","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"By default, Finch stores arrays in column major order (first index fast). When the storage order of an array in a Finch expression corresponds to the loop order, we call this concordant iteration. For example, the following expression represents a concordant traversal of a sparse matrix, as the outer loops access the higher levels of the fiber tree:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = Fiber!(Dense(SparseList(Element(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\ns = Scalar(0.0)\n@finch for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\n(s = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"We can investigate the generated code with @finch_code.  This code iterates over only the nonzeros in order. If our matrix is m × n with nnz nonzeros, this takes O(n + nnz) time.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\nquote\n    s = ex.body.body.lhs.tns.bind\n    s_val = s.val\n    A_lvl = ex.body.body.rhs.tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    for j_3 = 1:A_lvl.shape\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n        A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(A_lvl_2_i1, A_lvl_2.shape)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_2.idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_2.idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while i <= phase_stop\n                A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                if A_lvl_2_i == phase_stop_2\n                    A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                    s_val = A_lvl_3_val_2 + s_val\n                    A_lvl_2_q += 1\n                end\n                i = phase_stop_2 + 1\n            end\n        end\n    end\n    (s = (Scalar){0.0, Float64}(s_val),)\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"When the loop order does not correspond to storage order, we call this discordant iteration. For example, if we swap the loop order in the above example, then Finch needs to randomly access each sparse column for each row i. We end up needing to find each (i, j) pair because we don't know whether it will be zero until we search for it. In all, this takes time O(n * m * log(nnz)), much less efficient! We shouldn't randomly access sparse arrays unless we really need to and they support it efficiently!","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Note the double for loop in the following code","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code for i=_, j=_ ; s[] += A[i, j] end # DISCORDANT, DO NOT DO THIS\n\n# output\n\nquote\n    s = ex.body.body.lhs.tns.bind\n    s_val = s.val\n    A_lvl = ex.body.body.rhs.tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_lvl_2.shape\n        for j_3 = 1:A_lvl.shape\n            A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n            A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n            A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n            if A_lvl_2_q < A_lvl_2_q_stop\n                A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n            else\n                A_lvl_2_i1 = 0\n            end\n            phase_stop = min(i_3, A_lvl_2_i1)\n            if phase_stop >= i_3\n                s_2 = i_3\n                if A_lvl_2.idx[A_lvl_2_q] < i_3\n                    A_lvl_2_q = Finch.scansearch(A_lvl_2.idx, i_3, A_lvl_2_q, A_lvl_2_q_stop - 1)\n                end\n                while s_2 <= phase_stop\n                    A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                    phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_2\n                        A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                        s_val = A_lvl_3_val_2 + s_val\n                        A_lvl_2_q += 1\n                    end\n                    s_2 = phase_stop_2 + 1\n                end\n            end\n        end\n    end\n    (s = (Scalar){0.0, Float64}(s_val),)\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"TL;DR: As a quick heuristic, if your array indices are all in alphabetical order, then the loop indices should be reverse alphabetical.","category":"page"},{"location":"performance/#Appropriate-Fill-Values","page":"Performance Tips","title":"Appropriate Fill Values","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"The @finch macro requires the user to specify an output format. This is the most flexibile approach, but can sometimes lead to densification unless the output fill value is appropriate for the computation.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"For example, if A is m × n with nnz nonzeros, the following Finch kernel will densify B, filling it with m * n stored values:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = Fiber!(Dense(SparseList(Element(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Fiber!(Dense(SparseList(Element(0.0)))) #DO NOT DO THIS, B has the wrong fill value\n@finch (B .= 0; for j=_, i=_; B[i, j] = A[i, j] + 1 end)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Since A is filled with 0.0, adding 1 to the fill value produces 1.0. However, B can only represent a fill value of 0.0. Instead, we should specify 1.0 for the fill.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = Fiber!(Dense(SparseList(Element(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Fiber!(Dense(SparseList(Element(1.0))))\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + 1 end)\ncountstored(B)\n\n# output\n\n5","category":"page"},{"location":"performance/#Static-Versus-Dynamic-Values","page":"Performance Tips","title":"Static Versus Dynamic Values","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"In order to skip some computations, Finch must be able to determine the value of program variables. Continuing our above example, if we obscure the value of 1 behind a variable x, Finch can only determine that x has type Int, not that it is 1.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = Fiber!(Dense(SparseList(Element(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = Fiber!(Dense(SparseList(Element(1.0))))\nx = 1 #DO NOT DO THIS, Finch cannot see the value of x anymore\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + x end)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"However, there are some situations where you may want a value to be dynamic. For example, consider the function saxpy(x, a, y) = x .* a .+ y. Because we do not know the value of a until we run the function, we should treat it as dynamic, and the following implementation is reasonable:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"function saxpy(x, a, y)\n    z = Fiber!(SparseList(Element(0.0)))\n    @finch (z .= 0; for i=_; z[i] = a * x[i] + y[i] end)\nend","category":"page"},{"location":"performance/#Use-Known-Functions","page":"Performance Tips","title":"Use Known Functions","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Unless you declare the properties of your functions using Finch's Custom Functions interface, Finch doesn't know how they work. For example, using a lambda obscures the meaning of *.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = Fiber!(Dense(SparseList(Element(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = ones(4, 3)\nC = Scalar(0.0)\nf(x, y) = x * y # DO NOT DO THIS, Obscures *\n@finch (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end)\n\n# output\n\n(C = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Checking the generated code, we see that this code is indeed densifying (notice the for-loop which repeatedly evaluates f(B[i, j], 0.0)).","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end)\n\n# output\n\nquote\n    C = (ex.bodies[1]).tns.bind\n    A_lvl = ((ex.bodies[2]).body.body.rhs.args[1]).tns.bind.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    B = ((ex.bodies[2]).body.body.rhs.args[2]).tns.bind\n    sugar_1 = size(B)\n    B_mode1_stop = sugar_1[1]\n    B_mode2_stop = sugar_1[2]\n    B_mode1_stop == A_lvl_2.shape || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode1_stop) != $(A_lvl_2.shape))\"))\n    B_mode2_stop == A_lvl.shape || throw(DimensionMismatch(\"mismatched dimension limits ($(B_mode2_stop) != $(A_lvl.shape))\"))\n    C_val = 0\n    for j_4 = 1:B_mode2_stop\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_4\n        A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(B_mode1_stop, A_lvl_2_i1)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_2.idx[A_lvl_2_q] < 1\n                A_lvl_2_q = Finch.scansearch(A_lvl_2.idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while i <= phase_stop\n                A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                if A_lvl_2_i == phase_stop_2\n                    for i_6 = i:phase_stop_2 - 1\n                        C_val = f(0.0, B[i_6, j_4]) + C_val\n                    end\n                    A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                    C_val = C_val + f(A_lvl_3_val_2, B[phase_stop_2, j_4])\n                    A_lvl_2_q += 1\n                else\n                    for i_8 = i:phase_stop_2\n                        C_val = f(0.0, B[i_8, j_4]) + C_val\n                    end\n                end\n                i = phase_stop_2 + 1\n            end\n        end\n        phase_start_3 = max(1, 1 + A_lvl_2_i1)\n        if B_mode1_stop >= phase_start_3\n            for i_10 = phase_start_3:B_mode1_stop\n                C_val = f(0.0, B[i_10, j_4]) + C_val\n            end\n        end\n    end\n    (C = (Scalar){0.0, Float64}(C_val),)\nend\n","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"CurrentModule = Finch","category":"page"},{"location":"internals/#Finch-Compilation","page":"Internals","title":"Finch Compilation","text":"","category":"section"},{"location":"internals/#Finch-Notation","page":"Internals","title":"Finch Notation","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"FinchNode\ncached\nfinch_leaf","category":"page"},{"location":"internals/#Finch.FinchNotation.FinchNode","page":"Internals","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node. Finch uses a variant of Concrete Index Notation as an intermediate representation. \n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"internals/#Finch.FinchNotation.cached","page":"Internals","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.finch_leaf","page":"Internals","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Program-Instances","page":"Internals","title":"Program Instances","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Finch relies heavily on Julia's metaprogramming capabilities ( macros and generated functions in particular) to produce code. To review briefly, a macro allows us to inspect the syntax of it's arguments and generate replacement syntax. A generated function allows us to inspect the type of the function arguments and produce code for a function body.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"In normal Finch usage, we might call Finch as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> C = Fiber!(SparseList(Element(0)));\n\njulia> A = Fiber!(SparseList(Element(0)), [0, 2, 0, 0, 3]);\n\n\njulia> B = Fiber!(Dense(Element(0)), [11, 12, 13, 14, 15]);\n\njulia> @finch (C .= 0; for i=_; C[i] = A[i] * B[i] end);\n\n\njulia> C\nSparseList (0) [1:5]\n├─[2]: 24\n├─[5]: 45","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The @macroexpand macro allows us to see the result of applying a macro. Let's examine what happens when we use the @finch macro (we've stripped line numbers from the result to clean it up):","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> (@macroexpand @finch (C .= 0; for i=_; C[i] = A[i] * B[i] end)) |> Finch.striplines |> Finch.regensym\nquote\n    _res_1 = (Finch.execute)((Finch.FinchNotation.block_instance)((Finch.FinchNotation.declare_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), literal_instance(0)), begin\n                    let i = index_instance(i)\n                        (Finch.FinchNotation.loop_instance)(i, Finch.FinchNotation.Dimensionless(), (Finch.FinchNotation.assign_instance)((Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:C), (Finch.FinchNotation.finch_leaf_instance)(C)), (Finch.FinchNotation.updater_instance)(), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.literal_instance)(Finch.FinchNotation.initwrite), (Finch.FinchNotation.call_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:*), (Finch.FinchNotation.finch_leaf_instance)(*)), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:A), (Finch.FinchNotation.finch_leaf_instance)(A)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.tag_instance)(variable_instance(:B), (Finch.FinchNotation.finch_leaf_instance)(B)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.tag_instance)(variable_instance(:i), (Finch.FinchNotation.finch_leaf_instance)(i))))))\n                    end\n                end), (;))\n    begin\n        if Finch.haskey(_res_1, :C)\n            C = _res_1[:C]\n        end\n    end\n    begin\n        _res_1\n    end\nend\n","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"In the above output, @finch creates an AST of program instances, then calls Finch.execute on it. A program instance is a struct that contains the program to be executed along with its arguments. Although we can use the above constructors (e.g. loop_instance) to make our own program instance, it is most convenient to use the unexported macro Finch.finch_program_instance:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> using Finch: @finch_program_instance\n\njulia> prgm = Finch.@finch_program_instance (C .= 0; for i=_; C[i] = A[i] * B[i] end)\nblock_instance(declare_instance(tag_instance(:variable_instance(:C), Fiber(SparseList{Int64, Int64}(Element{0, Int64, Int64, Vector{Int64}}([24, 45]), 5, [1, 3], [2, 5]))), literal_instance(0)), loop_instance(index_instance(i), Finch.FinchNotation.Dimensionless(), assign_instance(access_instance(tag_instance(:variable_instance(:C), Fiber(SparseList{Int64, Int64}(Element{0, Int64, Int64, Vector{Int64}}([24, 45]), 5, [1, 3], [2, 5]))), updater_instance(), tag_instance(:variable_instance(:i), index_instance(i))), literal_instance(initwrite), call_instance(tag_instance(:variable_instance(:*), literal_instance(*)), access_instance(tag_instance(:variable_instance(:A), Fiber(SparseList{Int64, Int64}(Element{0, Int64, Int64, Vector{Int64}}([2, 3]), 5, [1, 3], [2, 5]))), reader_instance(), tag_instance(:variable_instance(:i), index_instance(i))), access_instance(tag_instance(:variable_instance(:B), Fiber(Dense{Int64}(Element{0, Int64, Int64, Vector{Int64}}([11, 12, 13, 14, 15]), 5))), reader_instance(), tag_instance(:variable_instance(:i), index_instance(i)))))))","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"As we can see, our program instance contains not only the AST to be executed, but also the data to execute the program with. The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. We can run our program using Finch.execute, which returns a NamedTuple of outputs.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> typeof(prgm)\nFinch.FinchNotation.BlockInstance{Tuple{Finch.FinchNotation.DeclareInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Fiber{SparseListLevel{Int64, Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.LiteralInstance{0}}, Finch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.Dimensionless, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:C}, Fiber{SparseListLevel{Int64, Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.UpdaterInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.initwrite}, Finch.FinchNotation.CallInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:*}, Finch.FinchNotation.LiteralInstance{*}}, Tuple{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:A}, Fiber{SparseListLevel{Int64, Int64, Vector{Int64}, Vector{Int64}, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:B}, Fiber{DenseLevel{Int64, ElementLevel{0, Int64, Int64, Vector{Int64}}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.TagInstance{Finch.FinchNotation.VariableInstance{:i}, Finch.FinchNotation.IndexInstance{:i}}}}}}}}}}\n\njulia> C = Finch.execute(prgm).C\nSparseList (0) [1:5]\n├─[2]: 24\n├─[5]: 45","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"This functionality is sufficient for building finch kernels programatically. For example, if we wish to define a function pointwise_sum() that takes the pointwise sum of a variable number of vector inputs, we might implement it as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> function pointwise_sum(As...)\n           B = Fiber!(Dense(Element(0)))\n           isempty(As) && return B\n           i = Finch.FinchNotation.index_instance(:i)\n           A_vars = [Finch.FinchNotation.tag_instance(Finch.FinchNotation.variable_instance(Symbol(:A, n)), As[n]) for n in 1:length(As)]\n           #create a list of variable instances with different names to hold the input tensors\n           ex = @finch_program_instance 0\n           for A_var in A_vars\n               ex = @finch_program_instance $A_var[i] + $ex\n           end\n           prgm = @finch_program_instance (B .= 0; for i=_; B[i] = $ex end)\n           return Finch.execute(prgm).B\n       end\npointwise_sum (generic function with 1 method)\n\njulia> pointwise_sum([1, 2], [3, 4])\nDense [1:2]\n├─[1]: 4\n├─[2]: 6\n","category":"page"},{"location":"internals/#Virtual-Tensor-Methods","page":"Internals","title":"Virtual Tensor Methods","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"declare!\ninstantiate_reader\ninstantiate_updater\nfreeze!\ntrim!\nthaw!\nunfurl","category":"page"},{"location":"internals/#Finch.declare!","page":"Internals","title":"Finch.declare!","text":"declare!(tns, ctx, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.instantiate_reader","page":"Internals","title":"Finch.instantiate_reader","text":"instantiate_reader(tns, ctx, protos)\n\nReturn an object (usually a looplet nest) capable of reading the read-only virtual tensor tns.  As soon as a read-only tensor enters scope, each subsequent read access will be initialized with a separate call to instantiate_reader. protos is the list of protocols in each case.\n\nThe fallback for instantiate_reader will iteratively move the last element of protos into the arguments of a function. This allows fibers to specialize on the last arguments of protos rather than the first, as Finch is column major.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.instantiate_updater","page":"Internals","title":"Finch.instantiate_updater","text":"instantiate_updater(tns, ctx, protos...)\n\nReturn an object (usually a looplet nest) capable of updating the update-only virtual tensor tns.  As soon as an update only tensor enters scope, each subsequent update access will be initialized with a separate call to instantiate_updater.  protos is the list of protocols in each case.\n\nThe fallback for instantiate_updater will iteratively move the last element of protos into the arguments of a function. This allows fibers to specialize on the last arguments of protos rather than the first, as Finch is column major.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.freeze!","page":"Internals","title":"Finch.freeze!","text":"freeze!(tns, ctx)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.trim!","page":"Internals","title":"Finch.trim!","text":"trim!(tns, ctx)\n\nBefore returning a tensor from the finch program, trim any excess overallocated memory.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.thaw!","page":"Internals","title":"Finch.thaw!","text":"thaw!(tns, ctx)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.unfurl","page":"Internals","title":"Finch.unfurl","text":"unfurl(tns, ctx, ext, protos...)\n\nReturn an array object (usually a looplet nest) for lowering the virtual tensor tns. ext is the extent of the looplet. protos is the list of protocols that should be used for each index, but one doesn't need to unfurl all the indices at once.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Virtualization","page":"Internals","title":"Virtualization","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"TODO more on the way...","category":"page"},{"location":"internals/#Fiber-internals","page":"Internals","title":"Fiber internals","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Fiber levels are implemented using the following methods:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"default\ndeclare_level!\nassemble_level!\nreassemble_level!\nfreeze_level!\nlevel_ndims\nlevel_size\nlevel_axes\nlevel_eltype\nlevel_default","category":"page"},{"location":"internals/#Finch.default","page":"Internals","title":"Finch.default","text":"default(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the default value becomes the fill or background value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.declare_level!","page":"Internals","title":"Finch.declare_level!","text":"declare_level!(lvl, ctx, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.assemble_level!","page":"Internals","title":"Finch.assemble_level!","text":"assemble_level!(lvl, ctx, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.reassemble_level!","page":"Internals","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_default(lvl).\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.freeze_level!","page":"Internals","title":"Finch.freeze_level!","text":"freeze_level!(lvl, ctx, pos)\n\nFreeze all fibers in lvl. Positions 1:pos need freezing.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_ndims","page":"Internals","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_size","page":"Internals","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_axes","page":"Internals","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_eltype","page":"Internals","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_default","page":"Internals","title":"Finch.level_default","text":"level_default(::Type{Lvl})\n\nThe result of level_default(Lvl) defines default for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"CurrentModule = Finch","category":"page"},{"location":"fibers/#Level-Formats","page":"Array Formats","title":"Level Formats","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Finch implements a flexible array datastructure called a fiber. Fibers represent arrays as rooted trees, where the child of each node is selected by an array index. Finch is column major, so in an expression A[i_1, ..., i_N], the rightmost dimension i_N corresponds to the root level of the tree, and the leftmost dimension i_1 corresponds to the leaf level. When the array is dense, the leftmost dimension has stop 1. We can convert the matrix A to a fiber with the Fiber! constructor:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\njulia> A_fbr = Fiber!(Dense(Dense(Element(0.0))), A)\nDense [:,1:3]\n├─[:,1]: Dense [1:4]\n│ ├─[1]: 0.0\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,2]: Dense [1:4]\n│ ├─[1]: 0.0\n│ ├─[2]: 0.0\n│ ├─[3]: 0.0\n│ ├─[4]: 0.0\n├─[:,3]: Dense [1:4]\n│ ├─[1]: 4.4\n│ ├─[2]: 0.0\n│ ├─[3]: 5.5\n│ ├─[4]: 0.0","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"We refer to a node in the tree as a subfiber. All of the nodes at the same level are stored in the same datastructure, and disambiguated by an integer position.  In the above example, there are three levels: The rootmost level contains only one fiber, the root. The middle level has 3 subfibers, one for each column. The leafmost level has 12 subfibers, one for each element of the array.  For example, the first level is A_fbr.lvl, and we can represent it's third position as SubFiber(A_fbr.lvl.lvl, 3). The second level is A_fbr.lvl.lvl, and we can access it's 9th position as SubFiber(A_fbr.lvl.lvl.lvl, 9). For instructional purposes, you can use parentheses to call a fiber on an index to select among children of a fiber.","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> Finch.SubFiber(A_fbr.lvl.lvl, 3)\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> A_fbr[:, 3]\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> A_fbr(3)\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> Finch.SubFiber(A_fbr.lvl.lvl.lvl, 9)\n4.4\n\njulia> A_fbr[1, 3]\n4.4\n\njulia> A_fbr(3)(1)\n4.4\n","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"When we print the tree in text, positions are numbered from top to bottom. However, if we visualize our tree with the root at the top, positions range from left to right:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: Dense Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Because our array is sparse, (mostly zero, or another fill value), it would be more efficient to store only the nonzero values. In Finch, each level is represented with a different format. A sparse level only stores non-fill values. This time, we'll use a fiber constructor with sl (for \"SparseList of nonzeros\") instead of d (for \"Dense\"):","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = Fiber!(Dense(SparseList(Element(0.0))), A)\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:4]\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,2]: SparseList (0.0) [1:4]\n├─[:,3]: SparseList (0.0) [1:4]\n│ ├─[1]: 4.4\n│ ├─[3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: CSC Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Our Dense(SparseList(Element(0.0))) format is also known as \"CSC\" and is equivalent to SparseMatrixCSC. The fiber! function will perform a zero-cost copy between Finch fibers and sparse matrices, when available.  CSC is an excellent general-purpose representation when we expect most of the columns to have a few nonzeros. However, when most of the columns are entirely fill (a situation known as hypersparsity), it is better to compress the root level as well:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = Fiber!(SparseList(SparseList(Element(0.0))), A)\nSparseList (0.0) [:,1:3]\n├─[:,1]: SparseList (0.0) [1:4]\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,3]: SparseList (0.0) [1:4]\n│ ├─[1]: 4.4\n│ ├─[3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: DCSC Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Here we see that the entirely zero column has also been compressed. The SparseList(SparseList(Element(0.0))) format is also known as \"DCSC\".","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"The \"COO\" (or \"Coordinate\") format is often used in practice for ease of interchange between libraries. In an N-dimensional array A, COO stores N lists of indices I_1, ..., I_N where A[I_1[p], ..., I_N[p]] is the p^th stored value in column-major numbering. In Finch, COO is represented as a multi-index level, which can handle more than one index at once. We use curly brackets to declare the number of indices handled by the level:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = Fiber!(SparseCOO{2}(Element(0.0)), A)\nSparseCOO (0.0) [1:4,1:3]\n├─├─[2, 1]: 1.1\n├─├─[3, 1]: 2.2\n├─├─[4, 1]: 3.3\n├─├─[1, 3]: 4.4\n├─├─[3, 3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: COO Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"The COO format is compact and straightforward, but doesn't support random access. For random access, one should use the SparseHash format. A full listing of supported formats is described after a rough description of shared common internals of level, relating to types and storage.","category":"page"},{"location":"fibers/#Types-and-Storage-of-Level","page":"Array Formats","title":"Types and Storage of Level","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"All levels have a postype, typically denoted as Tp in the constructors, used for internal pointer types but accessible by the function:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"postype","category":"page"},{"location":"fibers/#Finch.postype","page":"Array Formats","title":"Finch.postype","text":"postype(lvl)\n\nReturn a position type with the same flavor as those used to store the positions of the fibers contained in lvl. The name position descends from the pos or position or pointer arrays found in many definitions of CSR or CSC. In Finch, positions should be data used to access either a subfiber or some other similar auxiliary data. Thus, we often end up iterating over positions.\n\n\n\n\n\n","category":"function"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Additionally, many levels have a Vp or Vi in their constructors; these stand for vector of element type Tp or Ti.  More generally, levels are paramterized by the types that they use for storage. By default, all levels use Vector, but a user  could could change any or all of the storage types of a fiber so that the fiber would be stored on a GPU or CPU or some combination thereof,  or eveni just via a vector with a different allocation mechanism.  The storage type should behave like AbstractArray  and needs to implement the usual abstract array functions and Base.resize!. See the tests for an example. ","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"When levels are constructed in short form as in the examples above, the index, position, and storage types are inferred from the level below. All the levels at the bottom of a Fiber (Element, Pattern, Repeater) specify an index type, position type, and storage type even if they don't need them. These are used by levels that take these as parameters. ","category":"page"},{"location":"fibers/#Move-to:-Copying-Fibers-to-a-new-storage-type.","page":"Array Formats","title":"Move to: Copying Fibers to a new storage type.","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"If one needs to copy a fiber to another fiber with a different storage type, one can use the moveto function, described below.","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"moveto","category":"page"},{"location":"fibers/#Finch.moveto","page":"Array Formats","title":"Finch.moveto","text":"moveto(fbr, memType)\n\nIf the fiber/level is not on the given memType, it creates a new version of this fiber on that memory type and copies the data in to it, according to the constructor memtype.\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Public-Functions","page":"Array Formats","title":"Public Functions","text":"","category":"section"},{"location":"fibers/#Fiber-Constructors","page":"Array Formats","title":"Fiber Constructors","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Fiber!\nfiber\nfiber!","category":"page"},{"location":"fibers/#Finch.Fiber!","page":"Array Formats","title":"Finch.Fiber!","text":"Fiber!(ctr, [arg])\n\nConstruct a fiber from a nest of levels. This function may allocate memory. Optionally, an argument may be specified to copy into the fiber. This expression allocates. Use fiber(arg) for a zero-cost copy, if available.\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Finch.fiber","page":"Array Formats","title":"Finch.fiber","text":"fiber(arr, default = zero(eltype(arr)))\n\nCopies an array-like object arr into a corresponding, similar Fiber datastructure. default is the default value to use for initialization and sparse compression.\n\nSee also: fiber!\n\nExamples\n\njulia> println(summary(fiber(sparse([1 0; 0 1]))))\n2×2 Fiber!(Dense(SparseList(Element(0))))\n\njulia> println(summary(fiber(ones(3, 2, 4))))\n3×2×4 Fiber!(Dense(Dense(Dense(Element(0.0)))))\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Finch.fiber!","page":"Array Formats","title":"Finch.fiber!","text":"fiber!(arr, default = zero(eltype(arr)))\n\nLike fiber, copies an array-like object arr into a corresponding, similar Fiber datastructure. However, fiber! reuses memory whenever possible, meaning arr may be rendered unusable.\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Level-Constructors","page":"Array Formats","title":"Level Constructors","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"DenseLevel\nElementLevel\nSparseListLevel\nSparseCOOLevel\nSparseHashLevel\nSparseTriangleLevel\nSparseByteMapLevel","category":"page"},{"location":"fibers/#Finch.DenseLevel","page":"Array Formats","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\njulia> ndims(Fiber!(Dense(Element(0.0))))\n1\n\njulia> ndims(Fiber!(Dense(Dense(Element(0.0)))))\n2\n\njulia> Fiber!(Dense(Dense(Element(0.0))), [1 2; 3 4])\nDense [:,1:2]\n├─[:,1]: Dense [1:2]\n│ ├─[1]: 1.0\n│ ├─[2]: 3.0\n├─[:,2]: Dense [1:2]\n│ ├─[1]: 2.0\n│ ├─[2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.ElementLevel","page":"Array Formats","title":"Finch.ElementLevel","text":"ElementLevel{D, [Tv=typeof(D), Tp=Int, Vv]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to D. D may optionally be given as the first argument.\n\nThe data is stored in a vector of type Vv with eltype(Vv) = Tv. The type Ti is the index type used to access Vv.\n\njulia> Fiber!(Dense(Element(0.0)), [1, 2, 3])\nDense [1:3]\n├─[1]: 1.0\n├─[2]: 2.0\n├─[3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseListLevel","page":"Array Formats","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Tp=Int], [Vp=Vector{Tp}], [Vi=Vector{Ti}]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. The types Vp and Vi are the types of the arrays used to store positions and indicies. \n\njulia> Fiber!(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseList (0.0) [1:3]\n├─[:,3]: SparseList (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> Fiber!(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nSparseList (0.0) [:,1:3]\n├─[:,1]: SparseList (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,3]: SparseList (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseCOOLevel","page":"Array Formats","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [Ti=Tuple{Int...}], [Tp=Int], [Tbl], [Vp]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTi is the type of the last N fiber indices, and Tp is the type used for positions in the level.\n\nThe type Tbl is an NTuple type where each entry k is a subtype AbstractVector{Ti[k]}.\n\nThe type Vp is the type for the pointer array.\n\njulia> Fiber!(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseCOO (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseCOO (0.0) [1:3]\n├─[:,3]: SparseCOO (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> Fiber!(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])\nSparseCOO (0.0) [1:3,1:3]\n├─├─[1, 1]: 10.0\n├─├─[2, 1]: 30.0\n├─├─[1, 3]: 20.0\n├─├─[3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseHashLevel","page":"Array Formats","title":"Finch.SparseHashLevel","text":"SparseHashLevel{[N], [Ti=Tuple{Int...}], [Tp=Int], [Tbl], [Vp] [VTpip]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl. The sparse hash level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A hash table is used to record which slices are stored. Optionally, dims are the sizes of the last dimensions.\n\nTi is the type of the last N fiber indices, and Tp is the type used for positions in the level. Tbl is the type of the dictionary used to do hashing, a subtype of Dict{Tuple{Tp, Ti}, Tp}. Finally, Vp stores the positions of subfibers and VTpip is a storage type that is a subtype of AbstractVector{Pair{Tuple{Tp, Ti}, Tp}}.\n\njulia> Fiber!(Dense(SparseHash{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseHash (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseHash (0.0) [1:3]\n├─[:,3]: SparseHash (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> Fiber!(SparseHash{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])\nSparseHash (0.0) [1:3,1:3]\n├─├─[1, 1]: 10.0\n├─├─[2, 1]: 30.0\n├─├─[1, 3]: 20.0\n├─├─[3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseTriangleLevel","page":"Array Formats","title":"Finch.SparseTriangleLevel","text":"SparseTriangleLevel{[N], [Ti=Int]}(lvl, [dims])\n\nThe sparse triangle level stores the upper triangle of N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n], where i_1 <= ... <= i_n.  A packed representation is used to encode the subfiber. Optionally, dims are the sizes of the last dimensions.\n\nTi is the type of the last N fiber indices.\n\njulia> Fiber!(SparseTriangle{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])\nSparseTriangle (0.0) [1:3]\n├─├─[1, 1]: 10.0\n├─├─[1, 2]: 0.0\n│ ⋮\n├─├─[2, 3]: 0.0\n├─├─[3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseByteMapLevel","page":"Array Formats","title":"Finch.SparseByteMapLevel","text":"SparseByteMapLevel{[Ti=Tuple{Int...}], [Tp=Int], [Vp] [BV]}(lvl, [dims])\n\nLike the SparseListLevel, but a dense bitmap is used to encode which slices are stored. This allows the ByteMap level to support random access.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level. \n\njulia> Fiber!(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseByteMap (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseByteMap (0.0) [1:3]\n├─[:,3]: SparseByteMap (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> Fiber!(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])\nSparseByteMap (0.0) [:,1:3]\n├─[:,1]: SparseByteMap (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,3]: SparseByteMap (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"CurrentModule = Finch","category":"page"},{"location":"semantics/#Finch-Notation","page":"Semantics","title":"Finch Notation","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch programs are written in Julia, but they are not Julia programs. Instead, they are an abstraction description of a tensor computation.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch programs are blocks of tensor operations, joined by control flow. Finch is an imperative language. The AST is separated into statements and expressions, where statements can modify the state of the program but expressions cannot.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"The core Finch expressions are:","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"literal e.g. 1, 1.0, nothing\nvalue e.g. x, y\nindex e.g. i, inside of for i = _; ... end\nvariable e.g. x, inside of (x = y; ...)\ncall e.g. op(args...)\naccess e.g. tns[idxs...]","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"And the core Finch statements are:","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"define e.g. var = val\ndeclare e.g. tns .= init\nassign e.g. lhs[idxs...] <<op>>= rhs\nloop e.g. for i = _; ... end\nsieve e.g. if cond; ... end\nblock e.g. begin ... end","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"literal\nvalue\nindex\nvariable\ncall\naccess\ndefine\nassign\nloop\nsieve\nblock","category":"page"},{"location":"semantics/#Finch.FinchNotation.literal","page":"Semantics","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.value","page":"Semantics","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.index","page":"Semantics","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.variable","page":"Semantics","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.call","page":"Semantics","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.access","page":"Semantics","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.define","page":"Semantics","title":"Finch.FinchNotation.define","text":"define(lhs, rhs)\n\nFinch AST statement that defines lhs as having the value rhs in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.assign","page":"Semantics","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). Overwriting is accomplished with the function overwrite(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.loop","page":"Semantics","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.sieve","page":"Semantics","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.block","page":"Semantics","title":"Finch.FinchNotation.block","text":"block(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn. If the body is not a block, replaces accesses to read-only tensors in the body with instantiatereader and accesses to update-only tensors in the body with instantiateupdater.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Scoping","page":"Semantics","title":"Scoping","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch programs are scoped. Scopes contain variable definitions and tensor declarations.  Loops and sieves introduce new scopes. The following program has four scopes, each of which is numbered to the left of the statements it contains.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"@finch begin\n1   y .= 0\n1   for j = _\n1   2   t .= 0\n1   2   for i = _\n1   2   3   t[] += A[i, j] * x[i]\n1   2   end\n1   2   for i = _\n1   2   4   y[i] += A[i, j] * t[]\n1   2   end\n1   end\nend","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Variables refer to their defined values in the innermost containing scope. If variables are undefined, they are assumed to have global scope (they may come from the surrounding program).","category":"page"},{"location":"semantics/#Tensor-Lifecycle","page":"Semantics","title":"Tensor Lifecycle","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Tensors have two modes: Read and Update. Tensors in read mode may be read, but not updated. Tensors in update mode may be updated, but not read. A tensor declaration initializes and possibly resizes the tensor, setting it to update mode. Also, Finch will automatically change the mode of tensors as they are used. However, tensors may only change their mode within scopes that contain their declaration. If a tensor has not been declared, it is assumed to have global scope.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Tensor declaration is different than variable definition. Declaring a tensor initializes the memory (usually to zero) and sets the tensor to update mode. Defining a tensor simply gives a name to that memory. A tensor may be declared multiple times, but it may only be defined once.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Tensors are assumed to be in read mode when they are defined.  Tensors must enter and exit scope in read mode. Finch inserts freeze and thaw statements to ensure that tensors are in the correct mode. Freezing a tensor prevents further updates and allows reads. Thawing a tensor allows further updates and prevents reads.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Tensor lifecycle statements consist of:","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"declare\nfreeze\nthaw","category":"page"},{"location":"semantics/#Finch.FinchNotation.declare","page":"Semantics","title":"Finch.FinchNotation.declare","text":"declare(tns, init)\n\nFinch AST statement that declares tns with an initial value init in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.freeze","page":"Semantics","title":"Finch.FinchNotation.freeze","text":"freeze(tns)\n\nFinch AST statement that freezes tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Finch.FinchNotation.thaw","page":"Semantics","title":"Finch.FinchNotation.thaw","text":"thaw(tns)\n\nFinch AST statement that thaws tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"semantics/#Dimensionalization","page":"Semantics","title":"Dimensionalization","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch loops have dimensions. Accessing a tensor with an unmodified loop index \"hints\" that the loop should have the same dimension as the corresponding axis of the tensor. Finch will automatically dimensionalize loops that are hinted by tensor accesses. One may refer to the automatically determined dimension using a variable named _ or :. ","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Similarly, tensor declarations also set the dimensions of a tensor. Accessing a tensor with an unmodified loop index \"hints\" that the tensor axis should have the same dimension as the corresponding loop. Finch will automatically dimensionalize declarations based on all updates up to the first read.  ","category":"page"},{"location":"semantics/#Array-Combinators","page":"Semantics","title":"Array Combinators","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch includes several array combinators that modify the behavior of arrays. For example, the OffsetArray type wraps an existing array, but shifts its indices. The PermissiveArray type wraps an existing array, but allows out-of-bounds reads and writes. When an array is accessed out of bounds, it produces Missing.","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Array combinators introduce some complexity to the tensor lifecycle, as wrappers may contain multiple or different arrays that could potentially be in different modes. Any array combinators used in a tensor access must reference a single global variable which holds the root array. The root array is the single array that gets declared, and changes modes from read to update, or vice versa.","category":"page"},{"location":"semantics/#Fancy-Indexing","page":"Semantics","title":"Fancy Indexing","text":"","category":"section"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Finch supports arbitrary indexing of arrays, but certain indexing operations have first class support through array combinators. Before dimensionalization, the following transformations are performed:","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"    A[i + c] =>        OffsetArray(A, c)[i]\n    A[i + j] =>      ToeplitzArray(A, 1)[i, j]\n       A[~i] => PermissiveArray(A, true)[i]","category":"page"},{"location":"semantics/","page":"Semantics","title":"Semantics","text":"Note that these transformations may change the behavior of dimensionalization, since they often result in unmodified loop indices (the index i will participate in dimensionalization, but an index expression like i + 1 will not).","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"We welcome contributions to Finch, and follow the Julia contributing guidelines.  If you use or want to use Finch and have a question or bug, please do file a Github issue!  If you want to contribute to Finch, please first file an issue to double check that there is interest from a contributor in the feature.","category":"page"},{"location":"CONTRIBUTING/#Versions","page":"Contributing","title":"Versions","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Finch is currently in a pre-release state. The API is not yet stable, and breaking changes may occur between minor versions. We follow semantic versioning and will release 1.0 when the API is stable. The main branch of the Finch repo is the most up-to-date development branch. While it is not stable, it should always pass tests.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Contributors will develop and test Finch from a local directory. Please see the Package documentation for more info, particularly the section on developing.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"To determine which version of Finch you have, run Pkg.status(\"Finch\") in the Julia REPL. If the installed version of Finch tracks a local path, the output will include the path like so:","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Status `~/.julia/environments/v1.9/Project.toml`\n  [9177782c] Finch v0.5.4 `~/Projects/Finch.jl`","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"If the installed version of Finch tracks a particular version (probably not what you want since it will not reflect local changes), the output will look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Status `~/.julia/environments/v1.8/Project.toml`\n  [9177782c] Finch v0.5.4","category":"page"},{"location":"CONTRIBUTING/#Utilities","page":"Contributing","title":"Utilities","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Finch include several scripts that can be executed directly, e.g. runtests.jl. These scripts are all have local Pkg environments. The scripts include convenience headers to automatically use their respective environments, so you won't need to worry about --project=. flags, etc.","category":"page"},{"location":"CONTRIBUTING/#Testing","page":"Contributing","title":"Testing","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"All pull requests should pass continuous integration testing before merging. The test suite has a few options, which are accessible through running the test suite directly as ./test/runtests.jl.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Finch compares compiler output against reference versions.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"If you have the appropriate permissions, you can run the FixBot github action on your PR branch to automatically generate output for both 32-bit and 64-bit builds.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"If you run the test suite directly you can pass the --overwrite flag to tell the test suite to overwrite the reference.  Because the reference output depends on the system word size, you'll need to generate reference output for 32-bit and 64-bit builds of Julia to get Finch to pass tests. The easiest way to do this is to run each 32-bit or 64-bit build of Julia on a system that supports it. You can Download multiple builds yourself or use juliaup to manage multiple versions. Using juliaup, it might look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"julia +release~x86 test/runtests.jl --overwrite\njulia +release~x64 test/runtests.jl --overwrite","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"The test suite takes a while to run. You can filter to only run a selection of test suites by specifying them as positional arguments, e.g.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"./test/runtests.jl constructors conversions representation","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"This information is summarized with ./test/runtests.jl --help","category":"page"},{"location":"CONTRIBUTING/#Benchmarking","page":"Contributing","title":"Benchmarking","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"The Finch test suite includes a benchmarking script that measures Finch performance on a variety of kernels. It also includes some scripts to help compare Finch performance on the feature branch to the main branch. To run the benchmarking script, run ./benchmarks/runbenchmarks.jl. To run the comparison script, run ./benchmarks/runjudge.jl. Both scripts take a while to run and generate a report at the end.","category":"page"},{"location":"CONTRIBUTING/#Documentation","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"The /docs directory includes Finch documentation in /src, and a built website in /build. You can build the website with ./docs/make.jl. You can run doctests with ./docs/test.jl, and fix doctests with ./docs/fix.jl, though both are included as part of the test suite.","category":"page"},{"location":"interop/#Using-Finch-with-Other-Languages","page":"C, C++, ...","title":"Using Finch with Other Languages","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"You can use Finch in other languages through our C interface! We also include convenience types for converting between 0-indexed and 1-indexed arrays.","category":"page"},{"location":"interop/#finch.h","page":"C, C++, ...","title":"finch.h","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"Refer to finch.h for detailed documentation. The public functions include a few shortcuts for constructing finch datatypes, as well as convenience functions for calling Julia from C. Refer also to the Julia documentation for more general advice. Refer to the tests for a working example of embedding in C. Note that calling finch_init will call jl_init, as well as initializing a few function pointers for the interface. Julia cannot see C references to Julia objects, so finch.h includes a few functions to introduce references on the Julia side that mirror C objects.","category":"page"},{"location":"interop/#Index-Compatibility","page":"C, C++, ...","title":"0-Index Compatibility","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"Julia, Matlab, etc. index arrays starting at 1. C, python, etc. index starting at 0. In a dense array, we can simply subtract one from the index, and in fact, this is what Julia will does under the hood when you pass a vector between C to Julia. ","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"However, for sparse array formats, it's not just a matter of subtracting one from the index, as the internal lists of indices, positions, etc all start from zero as well. To remedy the situation, Finch interoperates with the CIndices package, which exports a  type called CIndex. The internal representation of CIndex is one less than the value it represents, and we can use CIndex as the index or position type of a Finch array to represent arrays in other languages.","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"For example, if idx_c, ptr_c, and val_c are the internal arrays of a CSC matrix in a zero-indexed language, we can represent that matrix as a one-indexed Finch array without copying by calling","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"DocTestSetup = quote\n    using Finch\n    using CIndices\nend","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"julia> m = 4; n = 3; ptr_c = [0, 3, 3, 5]; idx_c = [1, 2, 3, 0, 2]; val_c = [1.1, 2.2, 3.3, 4.4, 5.5];\n\njulia> ptr_jl = reinterpret(CIndex{Int}, ptr_c)\n4-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 1\n 4\n 4\n 6\njulia> idx_jl = reinterpret(CIndex{Int}, idx_c)\n5-element reinterpret(CIndex{Int64}, ::Vector{Int64}):\n 2\n 3\n 4\n 1\n 3\njulia> A = Fiber(Dense(SparseList{CIndex{Int}, CIndex{Int}}(Element{0.0, Float64}(val_c), m, ptr_jl, idx_jl), n))\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:CIndex{Int64}(4)]\n│ ├─[CIndex{Int64}(2)]: 1.1\n│ ├─[CIndex{Int64}(3)]: 2.2\n│ ├─[CIndex{Int64}(4)]: 3.3\n├─[:,2]: SparseList (0.0) [1:CIndex{Int64}(4)]\n├─[:,3]: SparseList (0.0) [1:CIndex{Int64}(4)]\n│ ├─[CIndex{Int64}(1)]: 4.4\n│ ├─[CIndex{Int64}(3)]: 5.5","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"We can also convert between representations by by copying to or from CIndex fibers.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"CurrentModule = Finch","category":"page"},{"location":"fileio/#Finch-Tensor-File-Input/Output","page":"Tensor File I/O","title":"Finch Tensor File Input/Output","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"All of the file formats supported by Finch are listed below. Each format has a corresponding read and write function, and can be selected automatically based on the file extension with the following functions:","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"fread\nfwrite","category":"page"},{"location":"fileio/#Finch.fread","page":"Tensor File I/O","title":"Finch.fread","text":"fread(filename::AbstractString)\n\nRead the Finch fiber from a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.fwrite","page":"Tensor File I/O","title":"Finch.fwrite","text":"fwrite(filename::AbstractString, tns::Finch.Fiber)\n\nWrite the Finch fiber to a file using a file format determined by the file extension. The following file extensions are supported:\n\n.bsp.h5: Binsparse HDF5 file format\n.bspnpy: Binsparse NumPy and JSON subdirectory format\n.mtx: MatrixMarket .mtx text file format\n.ttx: TensorMarket .ttx text file format\n.tns: FROSTT .tns text file format\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Binsparse-Format-(.bsp)","page":"Tensor File I/O","title":"Binsparse Format (.bsp)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports the most recent revision of the Binsparse binary sparse tensor format, as well as the proposed v2.0 tensor extension. This is a good option for those who want an efficient way to transfer sparse tensors between supporting libraries and languages. The Binsparse format represents the tensor format as a JSON string in the underlying data container, which can be either HDF5 or a combination of NPY or JSON files. Binsparse arrays are stored 0-indexed.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"bspwrite\nbspread","category":"page"},{"location":"fileio/#Finch.bspwrite","page":"Tensor File I/O","title":"Finch.bspwrite","text":"bspwrite(::AbstractString, tns)\nbspwrite(::HDF5.File, tns)\nbspwrite(::NPYPath, tns)\n\nWrite the Finch tensor to a file using Binsparse file format.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.bspread","page":"Tensor File I/O","title":"Finch.bspread","text":"bspread(::AbstractString) bspread(::HDF5.File) bspread(::NPYPath)\n\nRead the Binsparse file into a Finch tensor.\n\nSupported file extensions are:\n\n.bsp.h5: HDF5 file format (HDF5 must be loaded)\n.bspnpy: NumPy and JSON directory format (NPZ must be loaded)\n\nwarning: Warning\n\n\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"fileio/#TensorMarket-(.mtx,-.ttx)","page":"Tensor File I/O","title":"TensorMarket (.mtx, .ttx)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports the MatrixMarket and TensorMarket formats, which prioritize readability and archiveability, storing matrices and tensors in plaintext.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"fttwrite\nfttread","category":"page"},{"location":"fileio/#Finch.fttwrite","page":"Tensor File I/O","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch fiber to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.fttread","page":"Tensor File I/O","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch fiber. The fiber will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"function"},{"location":"fileio/#FROSTT-(.tns)","page":"Tensor File I/O","title":"FROSTT (.tns)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports the FROSTT format for legacy codes that still use it.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"ftnswrite\nftnsread","category":"page"},{"location":"fileio/#Finch.ftnswrite","page":"Tensor File I/O","title":"Finch.ftnswrite","text":"ftnswrite(filename, tns)\n\nWrite a sparse Finch fiber to a FROSTT .tns file.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnswrite\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.ftnsread","page":"Tensor File I/O","title":"Finch.ftnsread","text":"ftnsread(filename)\n\nRead the contents of the FROSTT .tns file 'filename' into a Finch COO Fiber.\n\nTensorMarket must be loaded for this function to be available.\n\ndanger: Danger\nThis file format does not record the size or eltype of the tensor, and is provided for archival purposes only.\n\nSee also: tnsread\n\n\n\n\n\n","category":"function"},{"location":"usage/","page":"Usage","title":"Usage","text":"CurrentModule = Finch","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/#General-Purpose-(@finch)","page":"Usage","title":"General Purpose (@finch)","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Most users will want to use the @finch macro, which executes the given program immediately in the given scope. The program will be JIT-compiled on the first call to @finch with the given array argument types. If the array arguments to @finch are type stable, the program will be JIT-compiled when the surrounding function is compiled.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Very often, the best way to inspect Finch compiler behavior is through the @finch_code macro, which prints the generated code instead of executing it.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"@finch\n@finch_code","category":"page"},{"location":"usage/#Finch.@finch","page":"Usage","title":"Finch.@finch","text":"@finch [options...] prgm\n\nRun a finch program prgm. The syntax for a finch program is a set of nested loops, statements, and branches over pointwise array assignments. For example, the following program computes the sum of two arrays A = B + C:\n\nA .= 0\n@finch for i = _\n    A[i] = B[i] + C[i]\nend\n\nFinch programs are composed using the following syntax:\n\narr .= 0: an array declaration initializing arr to zero.\narr[inds...]: an array access, the array must be a variable and each index may be another finch expression.\nx + y, f(x, y): function calls, where x and y are finch expressions.\narr[inds...] = ex: an array assignment expression, setting arr[inds] to the value of ex.\narr[inds...] += ex: an incrementing array expression, adding ex to arr[inds]. *, &, |, are supported.\narr[inds...] <<min>>= ex: a incrementing array expression with a custom operator, e.g. <<min>> is the minimum operator.\nfor i = _ body end: a loop over the index i, where _ is computed from array access with i in body.\nif cond body end: a conditional branch that executes only iterations where cond is true.\n\nSymbols are used to represent variables, and their values are taken from the environment. Loops introduce index variables into the scope of their bodies.\n\nFinch uses the types of the arrays and symbolic analysis to discover program optimizations. If B and C are sparse array types, the program will only run over the nonzeros of either. \n\nSemantically, Finch programs execute every iteration. However, Finch can use sparsity information to reliably skip iterations when possible.\n\noptions are optional keyword arguments:\n\nalgebra: the algebra to use for the program. The default is DefaultAlgebra().\nmode: the optimization mode to use for the program. The default is fastfinch.\nctx: the context to use for the program. The default is a LowerJulia context with the given options.\n\nSee also: @finch_code\n\n\n\n\n\n","category":"macro"},{"location":"usage/#Finch.@finch_code","page":"Usage","title":"Finch.@finch_code","text":"@finch_code [options...] prgm\n\nReturn the code that would be executed in order to run a finch program prgm.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"usage/#Ahead-Of-Time-(@finch_kernel)","page":"Usage","title":"Ahead Of Time (@finch_kernel)","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"While @finch is the recommended way to use Finch, it is also possible to run finch ahead-of-time. The @finch_kernel macro generates a function definition ahead-of-time, which can be evaluated and then called later.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"There are several reasons one might want to do this:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If we want to make tweaks to the Finch implementation, we can directly modify the source code of the resulting function.\nWhen benchmarking Finch functions, we can easily and reliably ensure the benchmarked code is inferrable.\nIf we want to use Finch to generate code but don't want to include Finch as a dependency in our project, we can use @finch_kernel to generate the functions ahead of time and copy and paste the generated code into our project.  Consider automating this workflow to keep the kernels up to date!","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"    @finch_kernel","category":"page"},{"location":"usage/#Finch.@finch_kernel","page":"Usage","title":"Finch.@finch_kernel","text":"@finch_kernel [options...] fname(args...) = prgm\n\nReturn a definition for a function named fname which executes @finch prgm on the arguments args. args should be a list of variables holding representative argument instances or types.\n\nSee also: @finch\n\n\n\n\n\n","category":"macro"},{"location":"usage/","page":"Usage","title":"Usage","text":"As an example, the following code generates an spmv kernel definition, evaluates the definition, and then calls the kernel several times.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"let\n    A = Fiber!(Dense(SparseList(Element(0.0))))\n    x = Fiber!(Dense(Element(0.0)))\n    y = Fiber!(Dense(Element(0.0)))\n    def = @finch_kernel function spmv(y, A, x)\n        y .= 0.0\n        for j = _, i = _\n            y[i] += A[i, j] * x[j]\n        end\n    end\n    eval(def)\nend\n\nfunction main()\n    for i = 1:10\n        A2 = Fiber!(Dense(SparseList(Element(0.0))), fsprand((10, 10), 0.1))\n        x2 = Fiber!(Dense(Element(0.0)), rand(10))\n        y2 = Fiber!(Dense(Element(0.0)))\n        spmv(y2, A2, x2)\n    end\nend\n\nmain()","category":"page"},{"location":"README/#Finch.jl","page":"Finch.jl","title":"Finch.jl","text":"","category":"section"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"[docs]:https://willow-ahrens.github.io/Finch.jl/stable [ddocs]:https://willow-ahrens.github.io/Finch.jl/dev [ci]:https://github.com/willow-ahrens/Finch.jl/actions/workflows/CI.yml?query=branch%3Amain [cov]:https://codecov.io/gh/willow-ahrens/Finch.jl [tool]:https://mybinder.org/v2/gh/willow-ahrens/Finch.jl/gh-pages?labpath=dev%2Finteractive.ipynb","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"[docsico]:https://img.shields.io/badge/docs-stable-blue.svg [ddocsico]:https://img.shields.io/badge/docs-dev-blue.svg [ciico]:https://github.com/willow-ahrens/Finch.jl/actions/workflows/CI.yml/badge.svg?branch=main [covico]:https://codecov.io/gh/willow-ahrens/Finch.jl/branch/main/graph/badge.svg [toolico]:https://mybinder.org/badgelogo.svg","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Documentation Build Status Try It Online!\n[![][docsico]][docs] [![][ddocsico]][ddocs] [![][ciico]][ci] [![][covico]][cov] [![][tool_ico]][tool]","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Finch is a adaptable Julia-to-Julia compiler for loop nests over sparse or structured multidimensional arrays.  Finch allows you to write for-loops as if they are dense, but compile them to be sparse! Finch compiles the loops based on the structure of the data. The compiler takes care of applying rules like x * 0 => 0 and the like to avoid redundant computation. ","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Features Syntax (e.g. ...)\nMajor Sparse Formats (CSC, CSF, COO, Hash, Bytemap, Dense Triangular) Fiber!(Dense(SparseList(Element(0.0)))\nRLE (Run Length Encoding) and Sparse RLE Fiber!(Dense(RepeatRLE(0.0)))\nArbitrary Fill Values Other Than Zero Fiber!(SparseList(Element(1.0)))\nArbitrary Operators x[] <<min>>= y[i] + z[i]\nMultiple Outputs x[] <<min>>= y; z[] <<max>>=y\nMulticore Parallelism for i = parallel(1:100)\nConditionals if dist[] < best_dist[]\nConvolution A[i + j]\nConcatenation coalesce(A[~i], B[~i - size(A, 1)])","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"In addition to supporting sparse arrays, Finch can also handle custom operators and fill values other than zero, runs of repeated values, or even special structures such as clustered nonzeros or triangular patterns. Finch also supports if-statements and custom user types and functions.  Users can add rewrite rules to inform the compiler about any special user-defined properties or optimizations.  You can even modify indexing expressions to express sparse convolution, or to describe windows into structured arrays.","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"As an example, here's a program which calculates the minimum, maximum, sum, and variance of a sparse vector, reading the vector only once, and only reading nonzero values:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"using Finch\n\nX = Fiber!(SparseList(Element(0.0)), fsprand((10,), 0.5))\nx_min = Scalar(Inf)\nx_max = Scalar(-Inf)\nx_sum = Scalar(0.0)\nx_var = Scalar(0.0)\n@finch begin\n    for i = _\n        x = X[i]\n        x_min[] <<min>>= x\n        x_max[] <<max>>= x\n        x_sum[] += x\n        x_var[] += x * x\n    end\nend;","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Array formats in Finch are described recursively mode by mode.  Semantically, an array in Finch can be understood as a tree, where each level in the tree corresponds to a dimension and each edge corresponds to an index. For example, Fiber!(Dense(SparseList(Element(0.0)))) constructs a Float64 CSC-format sparse matrix, and Fiber!(SparseList(SparseList(Element(0.0)))) constructs a DCSC-format hypersparse matrix. As another example, here's a column-major sparse matrix-vector multiply:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"x = Fiber!(Dense(Element(0.0)), rand(42));\nA = Fiber!(Dense(SparseList(Element(0.0))), fsprand((42, 42), 0.1));\ny = Fiber!(Dense(Element(0.0)));\n@finch begin\n    y .= 0\n    for j=_, i=_\n        y[i] += A[i, j] * x[j]\n    end\nend;","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"At it's heart, Finch is powered by a new domain specific language for coiteration, breaking structured iterators into control flow units we call Looplets. Looplets are lowered progressively with several stages for rewriting and simplification.","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"The technologies enabling Finch are described in our manuscript.","category":"page"},{"location":"README/#Installation","page":"Finch.jl","title":"Installation","text":"","category":"section"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"At the Julia REPL, install the latest stable version by running:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"julia> using Pkg; Pkg.add(\"Finch\")","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"CurrentModule = Finch","category":"page"},{"location":"benchmark/#Benchmarking","page":"Benchmarking","title":"Benchmarking","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Julia code is nototoriously fussy to benchmark. We'll use BenchmarkTools.jl to automatically follow best practices for getting reliable julia benchmarks. We'll also follow the Julia Performance Tips.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Finch is even trickier to benchmark, for a few reasons:","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"The first time an @finch function is called, it is compiled, which takes an extra long time. @finch can also incur dynamic dispatch costs if the array types are not type stable. We can remedy this by using @finch_kernel, which simplifies benchmarking by compiling the function ahead of time, so it behaves like a normal Julia function. If you must use @finch, try to ensure that the code is type-stable.\nFinch fibers reuse memory from previous calls, so the first time a fiber is used in a finch function, it will allocate memory, but subsequent times not so much. If we want to benchmark the memory allocation, we can reconstruct the fiber each time. Otherwise, we can let the repeated executions of the kernel measure the non-allocating runtime.\nRuntime for sparse code often depends on the sparsity pattern, so it's important to benchmark with representative data. Using standard matrices or tensors from MatrixDepot.jl or TensorDepot.jl is a good way to do this.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"using Finch\nusing BenchmarkTools\nusing SparseArrays\nusing MatrixDepot","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Load a sparse matrix from MatrixDepot.jl and convert it to a Finch fiber","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"A = Fiber!(Dense(SparseList(Element(0.0))), matrixdepot(\"HB/west0067\"))\n(m, n) = size(A)\n\nx = Fiber!(Dense(Element(0.0)), rand(n))\ny = Fiber!(Dense(Element(0.0)))","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Dense [1:0]","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Construct a Finch kernel for sparse matrix-vector multiply","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"eval(@finch_kernel function spmv(y, A, x)\n    y .= 0\n    for j = _, i = _\n        y[i] += A[i, j] * x[j]\n    end\nend)","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"spmv (generic function with 1 method)","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Benchmark the kernel, ignoring allocation costs for y","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"@benchmark spmv($y, $A, $x)","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"BenchmarkTools.Trial: 10000 samples with 211 evaluations.\n Range (min … max):  355.450 ns … 517.379 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     358.014 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   359.839 ns ±   9.413 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▇▃▇█▃               ▁▂▃                                       ▂\n  █████▁▁▁▁▁▁▁▃▃▁▁▄▃▃████▄▆▄▅▆▄▅▅▄▅▆▅▆▆▇▇▇▆▆▅▆▆▄▆▅▆▅▅▄▆▅▅▆▃▆▄▃▆ █\n  355 ns        Histogram: log(frequency) by time        407 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"The @benchmark macro will benchmark a function in local scope, and it will run the function a few times to estimate the runtime. It will also try to avoid first-time compilation costs by running the function once before benchmarking it. Note the use of $ to interpolate the arrays into the benchmark, bringing them into the local scope.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"We can also benchmark the memory allocation of the kernel by constructing y in the benchmark kernel","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"@benchmark begin\n    y = Fiber!(Dense(Element(0.0)))\n    y = spmv(y, $A, $x).y\nend","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"BenchmarkTools.Trial: 10000 samples with 202 evaluations.\n Range (min … max):  387.168 ns …   3.504 μs  ┊ GC (min … max): 0.00% … 86.82%\n Time  (median):     391.297 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   403.645 ns ± 136.241 ns  ┊ GC (mean ± σ):  1.68% ±  4.34%\n\n   ▂▇█▇▅▂           ▂▃▂▂▂▂▁▁▁                                   ▂\n  ▆███████▅▄▄▃▅▄▄▃▄▇████████████▇▆▆▆▇█▇███▇▇▇▆▇█▇█▇▇▆▅▅▄▄▆▅▅▅▄▅ █\n  387 ns        Histogram: log(frequency) by time        452 ns <\n\n Memory estimate: 608 bytes, allocs estimate: 2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Finch","category":"page"},{"location":"#Finch","page":"Home","title":"Finch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch is an adaptable compiler for loop nests over sparse or otherwise structured arrays. Finch supports general sparsity as well as many specialized sparsity patterns, like clustered nonzeros, diagonals, or triangles.  In addition to zero, Finch supports optimizations over arbitrary fill values and operators, even run-length-compression.","category":"page"},{"location":"","page":"Home","title":"Home","text":"At it's heart, Finch is powered by a domain specific language for coiteration, breaking structured iterators into units we call Looplets. The Looplets are lowered progressively, leaving several opportunities to rewrite and simplify intermediate expressions.","category":"page"},{"location":"#Installation:","page":"Home","title":"Installation:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"Finch\")","category":"page"},{"location":"#Basics:","page":"Home","title":"Basics:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To begin, the following program sums the rows of a sparse matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Finch\nA = sprand(5, 5, 0.5)\ny = zeros(5)\n@finch begin\n    y .= 0\n    for i=_, j=_\n        y[i] += A[i, j]\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The @finch macro takes a block of code, and compiles it using the sparsity attributes of the arguments. In this case, A is a sparse matrix, so the compiler generates a sparse loop nest. The compiler takes care of applying rules like x * 0 => 0 during compilation to make the code more efficient.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can call @finch on any loop program, but it will only generate sparse code if the arguments are sparse. For example, the following program calculates the sum of the elements of a dense matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Finch\nA = rand(5, 5)\ns = Scalar(0.0)\n@finch begin\n    s .= 0\n    for i=_, j=_\n        s[] += A[i, j]\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can call @finch_code to see the generated code (since A is dense, the code is dense):","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> @finch_code for i=_, j=_ ; s[] += A[i, j] end\nquote\n    s = ex.body.body.lhs.tns.bind\n    s_val = s.val\n    A = ex.body.body.rhs.tns.bind\n    sugar_1 = size(A)\n    A_mode1_stop = sugar_1[1]\n    A_mode2_stop = sugar_1[2]\n    @warn \"Performance Warning: non-concordant traversal of A[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)\"\n    for i_3 = 1:A_mode1_stop\n        for j_3 = 1:A_mode2_stop\n            s_val = A[i_3, j_3] + s_val\n        end\n    end\n    (s = (Scalar){0, Int64}(s_val),)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"We're working on adding more documentation, for now take a look at the examples for linear algebra or graphs.","category":"page"}]
}
