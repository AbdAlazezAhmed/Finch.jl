var documenterSearchIndex = {"docs":
[{"location":"internals/","page":"Internals","title":"Internals","text":"CurrentModule = Finch","category":"page"},{"location":"internals/#Finch-Compilation-Pipeline","page":"Internals","title":"Finch Compilation Pipeline","text":"","category":"section"},{"location":"internals/#Finch-Notation","page":"Internals","title":"Finch Notation","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"FinchNode\nliteral\nindex\nvariable\nvirtual\nvalue\nloop\nassign\ncall\ncached\nreader\nupdater\ncreate\nmodify\ndeclare\nfreeze\nthaw\nforget\naccess\nsequence\nprotocol\nsieve\nfinch_leaf","category":"page"},{"location":"internals/#Finch.FinchNotation.FinchNode","page":"Internals","title":"Finch.FinchNotation.FinchNode","text":"FinchNode\n\nA Finch IR node. Finch uses a variant of Concrete Index Notation as an intermediate representation. \n\nThe FinchNode struct represents many different Finch IR nodes. The nodes are differentiated by a FinchNotation.FinchNodeKind enum.\n\n\n\n\n\n","category":"type"},{"location":"internals/#Finch.FinchNotation.literal","page":"Internals","title":"Finch.FinchNotation.literal","text":"literal(val)\n\nFinch AST expression for the literal value val.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.index","page":"Internals","title":"Finch.FinchNotation.index","text":"index(name)\n\nFinch AST expression for an index named name. Each index must be quantified by a corresponding loop which iterates over all values of the index.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.variable","page":"Internals","title":"Finch.FinchNotation.variable","text":"variable(name)\n\nFinch AST expression for a variable named name. The variable can be looked up in the context.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.virtual","page":"Internals","title":"Finch.FinchNotation.virtual","text":"virtual(val)\n\nFinch AST expression for an object val which has special meaning to the compiler. This type allows users to substitute their own ASTs, etc. into Finch expressions.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.value","page":"Internals","title":"Finch.FinchNotation.value","text":"value(val, type)\n\nFinch AST expression for host code val expected to evaluate to a value of type type.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.loop","page":"Internals","title":"Finch.FinchNotation.loop","text":"loop(idx, ext, body)\n\nFinch AST statement that runs body for each value of idx in ext. Tensors in body must have ranges that agree with ext.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.assign","page":"Internals","title":"Finch.FinchNotation.assign","text":"assign(lhs, op, rhs)\n\nFinch AST statement that updates the value of lhs to op(lhs, rhs). The tensors of lhs are returned.  Overwriting is accomplished with the function right(lhs, rhs) = rhs.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.call","page":"Internals","title":"Finch.FinchNotation.call","text":"call(op, args...)\n\nFinch AST expression for the result of calling the function op on args....\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.cached","page":"Internals","title":"Finch.FinchNotation.cached","text":"cached(val, ref)\n\nFinch AST expression val, equivalent to the quoted expression ref\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.reader","page":"Internals","title":"Finch.FinchNotation.reader","text":"reader()\n\nFinch AST expression for an access mode that is read-only.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.updater","page":"Internals","title":"Finch.FinchNotation.updater","text":"updater(mode)\n\nFinch AST expression for an access mode that may modify the tensor. The mode field specifies whether this access returns this tensor (initializing and finalizing) or modifies it in place.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.create","page":"Internals","title":"Finch.FinchNotation.create","text":"create()\n\nFinch AST expression for an \"allocating\" update. This access will initialize and freeze the tensor, and we can be sure that any values the tensor held before have been forgotten.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.modify","page":"Internals","title":"Finch.FinchNotation.modify","text":"modify()\n\nFinch AST expression for an \"in place\" update. The access will not initialize or freeze the tensor, but can modify it's existing values.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.declare","page":"Internals","title":"Finch.FinchNotation.declare","text":"declare(tns, init)\n\nFinch AST statement that declares tns with an initial value init in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.freeze","page":"Internals","title":"Finch.FinchNotation.freeze","text":"freeze(tns)\n\nFinch AST statement that freezes tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.thaw","page":"Internals","title":"Finch.FinchNotation.thaw","text":"thaw(tns)\n\nFinch AST statement that thaws tns in the current scope.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.forget","page":"Internals","title":"Finch.FinchNotation.forget","text":"forget(tns)\n\nFinch AST statement that marks the end of the lifetime of tns, at least until it is declared again.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.access","page":"Internals","title":"Finch.FinchNotation.access","text":"access(tns, mode, idx...)\n\nFinch AST expression representing the value of tensor tns at the indices idx.... The mode differentiates between reads or updates and whether the access is in-place.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.sequence","page":"Internals","title":"Finch.FinchNotation.sequence","text":"sequence(bodies...)\n\nFinch AST statement that executes each of it's arguments in turn. If the body is not a sequence, replaces accesses to read-only tensors in the body with getreader and accesses to update-only tensors in the body with getupdater.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.protocol","page":"Internals","title":"Finch.FinchNotation.protocol","text":"protocol(idx, mode)\n\nFinch AST expression marking an indexing expression idx with the protocol mode. These usually reside at the toplevel of an indexing expression to an access.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.sieve","page":"Internals","title":"Finch.FinchNotation.sieve","text":"sieve(cond, body)\n\nFinch AST statement that only executes body if cond is true.\n\n\n\n\n\n","category":"constant"},{"location":"internals/#Finch.FinchNotation.finch_leaf","page":"Internals","title":"Finch.FinchNotation.finch_leaf","text":"finch_leaf(x)\n\nReturn a terminal finch node wrapper around x. A convenience function to determine whether x should be understood by default as a literal, value, or virtual.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Program-Instances","page":"Internals","title":"Program Instances","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Finch relies heavily on Julia's metaprogramming capabilities ( macros and generated functions in particular) to produce code. To review briefly, a macro allows us to inspect the syntax of it's arguments and generate replacement syntax. A generated function allows us to inspect the type of the function arguments and produce code for a function body.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"In normal Finch usage, we might call Finch as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> C = @fiber(sl(e(0)));\n\njulia> A = @fiber(sl(e(0)), [0, 2, 0, 0, 3]);\n\njulia> B = @fiber(d(e(0)), [11, 12, 13, 14, 15]);\n\njulia> @finch (C .= 0; @loop i C[i] = A[i] * B[i]);\n\njulia> C\nSparseList (0) [1:5]\n├─[2]: 24\n├─[5]: 45","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The @macroexpand macro allows us to see the result of applying a macro. Let's examine what happens when we use the @finch macro (we've stripped line numbers from the result to clean it up):","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> (@macroexpand @finch (C .= 0; @loop i C[i] = A[i] * B[i])) |> Finch.striplines |> Finch.regensym\nquote\n    _res_1 = (Finch.execute)((Finch.FinchNotation.sequence_instance)((Finch.FinchNotation.declare_instance)((Finch.FinchNotation.variable_instance)(:C, (Finch.FinchNotation.finch_leaf_instance)(C)), literal_instance(0)), begin\n                    let i = index_instance(i)\n                        (Finch.FinchNotation.loop_instance)(i, index_instance(:), (Finch.FinchNotation.assign_instance)((Finch.FinchNotation.access_instance)((Finch.FinchNotation.variable_instance)(:C, (Finch.FinchNotation.finch_leaf_instance)(C)), (Finch.FinchNotation.updater_instance)((Finch.FinchNotation.create_instance)()), (Finch.FinchNotation.variable_instance)(:i, (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.literal_instance)((Finch.FinchNotation.initwrite)((Finch.default)(C))), (Finch.FinchNotation.call_instance)((Finch.FinchNotation.variable_instance)(:*, (Finch.FinchNotation.finch_leaf_instance)(*)), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.variable_instance)(:A, (Finch.FinchNotation.finch_leaf_instance)(A)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.variable_instance)(:i, (Finch.FinchNotation.finch_leaf_instance)(i))), (Finch.FinchNotation.access_instance)((Finch.FinchNotation.variable_instance)(:B, (Finch.FinchNotation.finch_leaf_instance)(B)), (Finch.FinchNotation.reader_instance)(), (Finch.FinchNotation.variable_instance)(:i, (Finch.FinchNotation.finch_leaf_instance)(i))))))\n                    end\n                end))\n    begin\n        C = Finch.get(_res_1, :C, C)\n    end\n    begin\n        _res_1\n    end\nend\n","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"In the above output, @finch creates an AST of program instances, then calls Finch.execute on it. A program instance is a struct that contains the program to be executed along with its arguments. Although we can use the above constructors (e.g. loop_instance) to make our own program instance, it is most convenient to use the unexported macro Finch.finch_program_instance:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> using Finch: @finch_program_instance\n\njulia> prgm = Finch.@finch_program_instance (C .= 0; @loop i C[i] = A[i] * B[i])\nsequence_instance(declare_instance(variable_instance(:C, C), literal_instance(0)), loop_instance(index_instance(i), index_instance(:), assign_instance(access_instance(variable_instance(:C, C), updater_instance(create_instance()), index_instance(i)), literal_instance(Finch.FinchNotation.InitWriter{0}()), call_instance(variable_instance(:*, *), access_instance(variable_instance(:A, A), reader_instance(), index_instance(i)), access_instance(variable_instance(:B, B), reader_instance(), index_instance(i))))))","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"As we can see, our program instance contains not only the AST to be executed, but also the data to execute the program with. The type of the program instance contains only the program portion; there may be many program instances with different inputs, but the same program type. We can run our program using Finch.execute, which returns a NamedTuple of outputs.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> typeof(prgm)\nFinch.FinchNotation.SequenceInstance{Tuple{Finch.FinchNotation.DeclareInstance{Finch.FinchNotation.VariableInstance{:C, Fiber{SparseListLevel{Int64, Int64, ElementLevel{0, Int64}}}}, Finch.FinchNotation.LiteralInstance{0}}, Finch.FinchNotation.LoopInstance{Finch.FinchNotation.IndexInstance{:i}, Finch.FinchNotation.IndexInstance{:(:)}, Finch.FinchNotation.AssignInstance{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.VariableInstance{:C, Fiber{SparseListLevel{Int64, Int64, ElementLevel{0, Int64}}}}, Finch.FinchNotation.UpdaterInstance{Finch.FinchNotation.CreateInstance}, Tuple{Finch.FinchNotation.IndexInstance{:i}}}, Finch.FinchNotation.LiteralInstance{Finch.FinchNotation.InitWriter{0}()}, Finch.FinchNotation.CallInstance{Finch.FinchNotation.VariableInstance{:*, Finch.FinchNotation.LiteralInstance{*}}, Tuple{Finch.FinchNotation.AccessInstance{Finch.FinchNotation.VariableInstance{:A, Fiber{SparseListLevel{Int64, Int64, ElementLevel{0, Int64}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.IndexInstance{:i}}}, Finch.FinchNotation.AccessInstance{Finch.FinchNotation.VariableInstance{:B, Fiber{DenseLevel{Int64, ElementLevel{0, Int64}}}}, Finch.FinchNotation.ReaderInstance, Tuple{Finch.FinchNotation.IndexInstance{:i}}}}}}}}}\n\njulia> C = Finch.execute(prgm).C\nSparseList (0) [1:5]\n├─[2]: 24\n├─[5]: 45","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"This functionality is sufficient for building finch kernels programatically. For example, if we wish to define a function pointwise_sum() that takes the pointwise sum of a variable number of vector inputs, we might implement it as follows:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"julia> function pointwise_sum(As...)\n           B = @fiber(d(e(0)))\n           isempty(As) && return B\n           i = Finch.FinchNotation.index_instance(:i)\n           A_vars = [Finch.FinchNotation.variable_instance(Symbol(:A, n), As[n]) for n in 1:length(As)]\n           #create a list of variable instances with different names to hold the input tensors\n           ex = @finch_program_instance 0\n           for A_var in A_vars\n               ex = @finch_program_instance $A_var[i] + $ex\n           end\n           prgm = @finch_program_instance (B .= 0; @loop i B[i] = $ex)\n           return Finch.execute(prgm).B\n       end\npointwise_sum (generic function with 1 method)\n\njulia> pointwise_sum([1, 2], [3, 4])\nDense [1:2]\n├─[1]: 4\n├─[2]: 6\n","category":"page"},{"location":"internals/#Virtualization","page":"Internals","title":"Virtualization","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"TODO more on the way...","category":"page"},{"location":"internals/#Tensor-Life-Cycle","page":"Internals","title":"Tensor Life Cycle","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Every virtual tensor must be in one of two modes: read-only mode or update-only mode. The following functions may be called on virtual tensors throughout their life cycle.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"declare!\nget_reader\nget_updater\nfreeze!\ntrim!\nthaw!","category":"page"},{"location":"internals/#Finch.declare!","page":"Internals","title":"Finch.declare!","text":"declare!(tns, ctx, init)\n\nDeclare the read-only virtual tensor tns in the context ctx with a starting value of init and return it. Afterwards the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.get_reader","page":"Internals","title":"Finch.get_reader","text":"get_reader(tns, ctx, protos...)\n\nReturn an object (usually a looplet nest) capable of reading the read-only virtual tensor tns.  As soon as a read-only tensor enters scope, each subsequent read access will be initialized with a separate call to get_reader. protos is the list of protocols in each case.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.get_updater","page":"Internals","title":"Finch.get_updater","text":"get_updater(tns, ctx, protos...)\n\nReturn an object (usually a looplet nest) capable of updating the update-only virtual tensor tns.  As soon as an update only tensor enters scope, each subsequent update access will be initialized with a separate call to get_updater.  protos is the list of protocols in each case.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.freeze!","page":"Internals","title":"Finch.freeze!","text":"freeze!(tns, ctx)\n\nFreeze the update-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is read-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.trim!","page":"Internals","title":"Finch.trim!","text":"trim!(tns, ctx)\n\nBefore returning a tensor from the finch program, trim any excess overallocated memory.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.thaw!","page":"Internals","title":"Finch.thaw!","text":"thaw!(tns, ctx)\n\nThaw the read-only virtual tensor tns in the context ctx and return it. Afterwards, the tensor is update-only.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Fiber-internals","page":"Internals","title":"Fiber internals","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Fiber levels are implemented using the following methods:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"default\ndeclare_level!\nassemble_level!\nreassemble_level!\nfreeze_level!\nlevel_ndims\nlevel_size\nlevel_axes\nlevel_eltype\nlevel_default","category":"page"},{"location":"internals/#Finch.default","page":"Internals","title":"Finch.default","text":"default(arr)\n\nReturn the initializer for arr. For SparseArrays, this is 0. Often, the default value becomes the fill or background value of a tensor.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.declare_level!","page":"Internals","title":"Finch.declare_level!","text":"declare_level!(lvl, ctx, pos, init)\n\nInitialize and thaw all fibers within lvl, assuming positions 1:pos were previously assembled and frozen. The resulting level has no assembled positions.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.assemble_level!","page":"Internals","title":"Finch.assemble_level!","text":"assemble_level!(lvl, ctx, pos, new_pos)\n\nAssemble and positions pos+1:new_pos in lvl, assuming positions 1:pos were previously assembled.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.reassemble_level!","page":"Internals","title":"Finch.reassemble_level!","text":"reassemble_level!(lvl, ctx, pos_start, pos_end)\n\nSet the previously assempled positions from pos_start to pos_end to level_default(lvl).\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.freeze_level!","page":"Internals","title":"Finch.freeze_level!","text":"freeze_level!(lvl, ctx, pos)\n\nFreeze all fibers in lvl. Positions 1:pos need freezing.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_ndims","page":"Internals","title":"Finch.level_ndims","text":"level_ndims(::Type{Lvl})\n\nThe result of level_ndims(Lvl) defines ndims for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_size","page":"Internals","title":"Finch.level_size","text":"level_size(lvl)\n\nThe result of level_size(lvl) defines the size of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_axes","page":"Internals","title":"Finch.level_axes","text":"level_axes(lvl)\n\nThe result of level_axes(lvl) defines the axes of all subfibers in the level lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_eltype","page":"Internals","title":"Finch.level_eltype","text":"level_eltype(::Type{Lvl})\n\nThe result of level_eltype(Lvl) defines eltype for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Finch.level_default","page":"Internals","title":"Finch.level_default","text":"level_default(::Type{Lvl})\n\nThe result of level_default(Lvl) defines default for all subfibers in a level of type Lvl.\n\n\n\n\n\n","category":"function"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"CurrentModule = Finch","category":"page"},{"location":"performance/#Performance-Tips-for-Finch","page":"Performance Tips","title":"Performance Tips for Finch","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"It's easy to ask Finch to run the same operation in different ways. However, different approaches have different performance. The right approach really depends on your particular situation. Here's a collection of general approaches that help Finch generate faster code in most cases.","category":"page"},{"location":"performance/#Concordant-Iteration","page":"Performance Tips","title":"Concordant Iteration","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"By default, Finch stores arrays in column major order (first index fast). When the storage order of an array in a Finch expression corresponds to the loop order, we call this concordant iteration. For example, the following expression represents a concordant traversal of a sparse matrix, as the outer loops access the higher levels of the fiber tree:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = @fiber(d(sl(e(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\ns = Scalar(0.0)\n@finch for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\n(s = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"We can investigate the generated code with @finch_code.  This code iterates over only the nonzeros in order. If our matrix is m × n with nnz nonzeros, this takes O(n + nnz) time.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code for j=_, i=_ ; s[] += A[i, j] end\n\n# output\n\nquote\n    s = ex.body.body.lhs.tns.tns\n    s_val = s.val\n    A_lvl = ex.body.body.rhs.tns.tns.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    for j_3 = 1:A_lvl.shape\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n        A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        phase_stop = min(A_lvl_2_i1, A_lvl_2.shape)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_2.idx[A_lvl_2_q] < 1\n                A_lvl_2_q = scansearch(A_lvl_2.idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while i <= phase_stop\n                A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                if A_lvl_2_i == phase_stop_2\n                    A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                    s_val = A_lvl_3_val_2 + s_val\n                    A_lvl_2_q += 1\n                end\n                i = phase_stop_2 + 1\n            end\n        end\n    end\n    (s = (Scalar){0.0, Float64}(s_val),)\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"When the loop order does not correspond to storage order, we call this discordant iteration. For example, if we swap the loop order in the above example, then Finch needs to randomly access each sparse column for each row i. We end up needing to find each (i, j) pair because we don't know whether it will be zero until we search for it. In all, this takes time O(n * m * log(nnz)), much less efficient! We shouldn't randomly access sparse arrays unless we really need to and they support it efficiently!","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Note the double for loop in the following code","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code for i=_, j=_ ; s[] += A[i, j] end # DISCORDANT, DO NOT DO THIS\n\n# output\n\nquote\n    s = ex.body.body.lhs.tns.tns\n    s_val = s.val\n    A_lvl = ex.body.body.rhs.tns.tns.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    for i_3 = 1:A_lvl_2.shape\n        for j_3 = 1:A_lvl.shape\n            A_lvl_q = (1 - 1) * A_lvl.shape + j_3\n            A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n            A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n            if A_lvl_2_q < A_lvl_2_q_stop\n                A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n            else\n                A_lvl_2_i1 = 0\n            end\n            phase_stop = min(A_lvl_2_i1, i_3)\n            if phase_stop >= i_3\n                s_3 = i_3\n                if A_lvl_2.idx[A_lvl_2_q] < i_3\n                    A_lvl_2_q = scansearch(A_lvl_2.idx, i_3, A_lvl_2_q, A_lvl_2_q_stop - 1)\n                end\n                while s_3 <= phase_stop\n                    A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                    phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                    if A_lvl_2_i == phase_stop_2\n                        A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                        s_val = A_lvl_3_val_2 + s_val\n                        A_lvl_2_q += 1\n                    end\n                    s_3 = phase_stop_2 + 1\n                end\n            end\n        end\n    end\n    (s = (Scalar){0.0, Float64}(s_val),)\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"TL;DR: As a quick heuristic, if your array indices are all in alphabetical order, then the loop indices should be reverse alphabetical.","category":"page"},{"location":"performance/#Appropriate-Fill-Values","page":"Performance Tips","title":"Appropriate Fill Values","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"The @finch macro requires the user to specify an output format. This is the most flexibile approach, but can sometimes lead to densification unless the output fill value is appropriate for the computation.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"For example, if A is m × n with nnz nonzeros, the following Finch kernel will densify B, filling it with m * n stored values:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = @fiber(d(sl(e(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = @fiber(d(sl(e(0.0)))) #DO NOT DO THIS, B has the wrong fill value\n@finch (B .= 0; for j=_, i=_; B[i, j] = A[i, j] + 1 end)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Since A is filled with 0.0, adding 1 to the fill value produces 1.0. However, B can only represent a fill value of 0.0. Instead, we should specify 1.0 for the fill.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = @fiber(d(sl(e(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = @fiber(d(sl(e(1.0))))\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + 1 end)\ncountstored(B)\n\n# output\n\n5","category":"page"},{"location":"performance/#Static-Versus-Dynamic-Values","page":"Performance Tips","title":"Static Versus Dynamic Values","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"In order to skip some computations, Finch must be able to determine the value of program variables. Continuing our above example, if we obscure the value of 1 behind a variable x, Finch can only determine that x has type Int, not that it is 1.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = @fiber(d(sl(e(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = @fiber(d(sl(e(1.0))))\nx = 1 #DO NOT DO THIS, Finch cannot see the value of x anymore\n@finch (B .= 1; for j=_, i=_; B[i, j] = A[i, j] + x end)\ncountstored(B)\n\n# output\n\n12","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"However, there are some situations where you may want a value to be dynamic. For example, consider the function saxpy(x, a, y) = x .* a .+ y. Because we do not know the value of a until we run the function, we should treat it as dynamic, and the following implementation is reasonable:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"function saxpy(x, a, y)\n    z = @fiber(sl(e(0.0)))\n    @finch (z .= 0; for i=_; z[i] = a * x[i] + y[i] end)\nend","category":"page"},{"location":"performance/#Use-Known-Functions","page":"Performance Tips","title":"Use Known Functions","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Unless you declare the properties of your functions using Finch's Custom Functions interface, Finch doesn't know how they work. For example, using a lambda obscures the meaning of *.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A = @fiber(d(sl(e(0.0))), fsparse(([2, 3, 4, 1, 3], [1, 1, 1, 3, 3]), [1.1, 2.2, 3.3, 4.4, 5.5], (4, 3)))\nB = ones(4, 3)\nC = Scalar(0.0)\nf(x, y) = x * y # DO NOT DO THIS, Obscures *\n@finch (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end)\n\n# output\n\n(C = Scalar{0.0, Float64}(16.5),)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Checking the generated code, we see that this code is indeed densifying (notice the for-loop which repeatedly evaluates f(B[i, j], 0.0)).","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@finch_code (C .= 0; for j=_, i=_; C[] += f(A[i, j], B[i, j]) end)\n\n# output\n\nquote\n    C = (ex.bodies[1]).tns.tns\n    A_lvl = ((ex.bodies[2]).body.body.rhs.args[1]).tns.tns.lvl\n    A_lvl_2 = A_lvl.lvl\n    A_lvl_3 = A_lvl_2.lvl\n    B = ((ex.bodies[2]).body.body.rhs.args[2]).tns.tns\n    sugar_1 = size(B)\n    B_mode1_stop = sugar_1[1]\n    B_mode2_stop = sugar_1[2]\n    A_lvl_2.shape == B_mode1_stop || throw(DimensionMismatch(\"mismatched dimension limits ($(A_lvl_2.shape) != $(B_mode1_stop))\"))\n    A_lvl.shape == B_mode2_stop || throw(DimensionMismatch(\"mismatched dimension limits ($(A_lvl.shape) != $(B_mode2_stop))\"))\n    C_val = 0\n    for j_4 = 1:A_lvl.shape\n        A_lvl_q = (1 - 1) * A_lvl.shape + j_4\n        A_lvl_2_q = A_lvl_2.ptr[A_lvl_q]\n        A_lvl_2_q_stop = A_lvl_2.ptr[A_lvl_q + 1]\n        if A_lvl_2_q < A_lvl_2_q_stop\n            A_lvl_2_i1 = A_lvl_2.idx[A_lvl_2_q_stop - 1]\n        else\n            A_lvl_2_i1 = 0\n        end\n        i = 1\n        phase_stop = min(A_lvl_2.shape, A_lvl_2_i1)\n        if phase_stop >= 1\n            i = 1\n            if A_lvl_2.idx[A_lvl_2_q] < 1\n                A_lvl_2_q = scansearch(A_lvl_2.idx, 1, A_lvl_2_q, A_lvl_2_q_stop - 1)\n            end\n            while i <= phase_stop\n                A_lvl_2_i = A_lvl_2.idx[A_lvl_2_q]\n                phase_stop_2 = min(phase_stop, A_lvl_2_i)\n                if A_lvl_2_i == phase_stop_2\n                    for i_6 = i:phase_stop_2 - 1\n                        C_val = f(0.0, B[i_6, j_4]) + C_val\n                    end\n                    A_lvl_3_val_2 = A_lvl_3.val[A_lvl_2_q]\n                    C_val = C_val + f(A_lvl_3_val_2, B[phase_stop_2, j_4])\n                    A_lvl_2_q += 1\n                else\n                    for i_8 = i:phase_stop_2\n                        C_val = f(0.0, B[i_8, j_4]) + C_val\n                    end\n                end\n                i = phase_stop_2 + 1\n            end\n            i = phase_stop + 1\n        end\n        phase_stop_3 = A_lvl_2.shape\n        if phase_stop_3 >= i\n            for i_10 = i:phase_stop_3\n                C_val = f(0.0, B[i_10, j_4]) + C_val\n            end\n        end\n    end\n    (C = (Scalar){0.0, Float64}(C_val),)\nend\n","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"CurrentModule = Finch","category":"page"},{"location":"fibers/#Level-Formats","page":"Array Formats","title":"Level Formats","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Finch implements a flexible array datastructure called a fiber. Fibers represent arrays as rooted trees, where the child of each node is selected by an array index. Finch is column major, so in an expression A[i_1, ..., i_N], the rightmost dimension i_N corresponds to the root level of the tree, and the leftmost dimension i_1 corresponds to the leaf level. When the array is dense, the leftmost dimension has stop 1. We can convert the matrix A to a fiber with the @fiber constructor:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A = [0.0 0.0 4.4; 1.1 0.0 0.0; 2.2 0.0 5.5; 3.3 0.0 0.0]\n4×3 Matrix{Float64}:\n 0.0  0.0  4.4\n 1.1  0.0  0.0\n 2.2  0.0  5.5\n 3.3  0.0  0.0\njulia> A_fbr = @fiber(d(d(e(0.0))), A)\nDense [:,1:3]\n├─[:,1]: Dense [1:4]\n│ ├─[1]: 0.0\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,2]: Dense [1:4]\n│ ├─[1]: 0.0\n│ ├─[2]: 0.0\n│ ├─[3]: 0.0\n│ ├─[4]: 0.0\n├─[:,3]: Dense [1:4]\n│ ├─[1]: 4.4\n│ ├─[2]: 0.0\n│ ├─[3]: 5.5\n│ ├─[4]: 0.0","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"We refer to a node in the tree as a subfiber. All of the nodes at the same level are stored in the same datastructure, and disambiguated by an integer position.  In the above example, there are three levels: The rootmost level contains only one fiber, the root. The middle level has 3 subfibers, one for each column. The leafmost level has 12 subfibers, one for each element of the array.  For example, the first level is A_fbr.lvl, and we can represent it's third position as SubFiber(A_fbr.lvl.lvl, 3). The second level is A_fbr.lvl.lvl, and we can access it's 9th position as SubFiber(A_fbr.lvl.lvl.lvl, 9). For instructional purposes, you can use parentheses to call a fiber on an index to select among children of a fiber.","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> Finch.SubFiber(A_fbr.lvl.lvl, 3)\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> A_fbr[:, 3]\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> A_fbr(3)\nDense [1:4]\n├─[1]: 4.4\n├─[2]: 0.0\n├─[3]: 5.5\n├─[4]: 0.0\n\njulia> Finch.SubFiber(A_fbr.lvl.lvl.lvl, 9)\n4.4\n\njulia> A_fbr[1, 3]\n4.4\n\njulia> A_fbr(3)(1)\n4.4\n","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"When we print the tree in text, positions are numbered from top to bottom. However, if we visualize our tree with the root at the top, positions range from left to right:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: Dense Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Because our array is sparse, (mostly zero, or another fill value), it would be more efficient to store only the nonzero values. In Finch, each level is represented with a different format. A sparse level only stores non-fill values. This time, we'll use a fiber constructor with sl (for \"SparseList of nonzeros\") instead of d (for \"Dense\"):","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = @fiber(d(sl(e(0.0))), A)\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:4]\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,2]: SparseList (0.0) [1:4]\n├─[:,3]: SparseList (0.0) [1:4]\n│ ├─[1]: 4.4\n│ ├─[3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: CSC Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Our d(sl(e(0.0))) format is also known as \"CSC\" and is equivalent to SparseMatrixCSC. The fiber! function will perform a zero-cost copy between Finch fibers and sparse matrices, when available.  CSC is an excellent general-purpose representation when we expect most of the columns to have a few nonzeros. However, when most of the columns are entirely fill (a situation known as hypersparsity), it is better to compress the root level as well:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = @fiber(sl(sl(e(0.0))), A)\nSparseList (0.0) [:,1:3]\n├─[:,1]: SparseList (0.0) [1:4]\n│ ├─[2]: 1.1\n│ ├─[3]: 2.2\n│ ├─[4]: 3.3\n├─[:,3]: SparseList (0.0) [1:4]\n│ ├─[1]: 4.4\n│ ├─[3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: DCSC Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"Here we see that the entirely zero column has also been compressed. The sl(sl(e(0.0))) format is also known as \"DCSC\".","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"The \"COO\" (or \"Coordinate\") format is often used in practice for ease of interchange between libraries. In an N-dimensional array A, COO stores N lists of indices I_1, ..., I_N where A[I_1[p], ..., I_N[p]] is the p^th stored value in column-major numbering. In Finch, COO is represented as a multi-index level, which can handle more than one index at once. We use curly brackets to declare the number of indices handled by the level:","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"julia> A_fbr = @fiber(sc{2}(e(0.0)), A)\nSparseCOO (0.0) [1:4,1:3]\n├─├─[2, 1]: 1.1\n├─├─[3, 1]: 2.2\n├─├─[4, 1]: 3.3\n├─├─[1, 3]: 4.4\n├─├─[3, 3]: 5.5","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"(Image: COO Format Index Tree)","category":"page"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"The COO format is compact and straightforward, but doesn't support random access. For random access, one should use the SparseHash format. A full listing of supported formats is described below:","category":"page"},{"location":"fibers/#Public-Functions","page":"Array Formats","title":"Public Functions","text":"","category":"section"},{"location":"fibers/#Fiber-Constructors","page":"Array Formats","title":"Fiber Constructors","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"@fiber\nfiber\nfiber!\nfiber_abbrev","category":"page"},{"location":"fibers/#Finch.@fiber","page":"Array Formats","title":"Finch.@fiber","text":"@fiber ctr [arg]\n\nConstruct a fiber using abbreviated level constructor names. To override abbreviations, expressions may be interpolated with $. For example, Fiber(DenseLevel(SparseListLevel(Element(0.0)))) can also be constructed as @fiber(sl(d(e(0.0)))). Consult the documentation for the helper function fiber_abbrev for a full listing of level format abbreviations.\n\nOptionally, an argument may be specified to copy into the fiber. This expression allocates. Use fiber(arg) for a zero-cost copy, if available.\n\n\n\n\n\n","category":"macro"},{"location":"fibers/#Finch.fiber","page":"Array Formats","title":"Finch.fiber","text":"fiber(arr, default = zero(eltype(arr)))\n\nCopies an array-like object arr into a corresponding, similar Fiber datastructure. default is the default value to use for initialization and sparse compression.\n\nSee also: fiber!\n\nExamples\n\njulia> println(summary(fiber(sparse([1 0; 0 1]))))\n2×2 @fiber(d(sl(e(0))))\n\njulia> println(summary(fiber(ones(3, 2, 4))))\n3×2×4 @fiber(d(d(d(e(0.0)))))\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Finch.fiber!","page":"Array Formats","title":"Finch.fiber!","text":"fiber!(arr, default = zero(eltype(arr)))\n\nLike fiber, copies an array-like object arr into a corresponding, similar Fiber datastructure. However, fiber! reuses memory whenever possible, meaning arr may be rendered unusable.\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Finch.fiber_abbrev","page":"Array Formats","title":"Finch.fiber_abbrev","text":"fiber_abbrev(l) = SparseListLevel.\n\n\n\n\n\nfiber_abbrev(sh) = SparseHashLevel.\n\n\n\n\n\nfiber_abbrev(sc) = SparseCOOLevel.\n\n\n\n\n\nfiber_abbrev(sbm) = SparseByteMapLevel.\n\n\n\n\n\nfiber_abbrev(svb) = SparseVBLLevel.\n\n\n\n\n\nfiber_abbrev(d) = DenseLevel.\n\n\n\n\n\nfiber_abbrev(rl) = RepeatRLELevel.\n\n\n\n\n\nfiber_abbrev(e) = ElementLevel.\n\n\n\n\n\nfiber_abbrev(p) = PatternLevel.\n\n\n\n\n\nfiber_abbrev(st) = SparseTriangleLevel.\n\n\n\n\n\n","category":"function"},{"location":"fibers/#Level-Constructors","page":"Array Formats","title":"Level Constructors","text":"","category":"section"},{"location":"fibers/","page":"Array Formats","title":"Array Formats","text":"DenseLevel\nElementLevel\nSparseListLevel\nSparseCOOLevel\nSparseHashLevel","category":"page"},{"location":"fibers/#Finch.DenseLevel","page":"Array Formats","title":"Finch.DenseLevel","text":"DenseLevel{[Ti=Int]}(lvl, [dim])\n\nA subfiber of a dense level is an array which stores every slice A[:, ..., :, i] as a distinct subfiber in lvl. Optionally, dim is the size of the last dimension. Ti is the type of the indices used to index the level.\n\nIn the @fiber constructor, d is an alias for DenseLevel.\n\njulia> ndims(@fiber(d(e(0.0))))\n1\n\njulia> ndims(@fiber(d(d(e(0.0)))))\n2\n\njulia> @fiber(d(d(e(0.0))), [1 2; 3 4])\nDense [:,1:2]\n├─[:,1]: Dense [1:2]\n│ ├─[1]: 1.0\n│ ├─[2]: 3.0\n├─[:,2]: Dense [1:2]\n│ ├─[1]: 2.0\n│ ├─[2]: 4.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.ElementLevel","page":"Array Formats","title":"Finch.ElementLevel","text":"ElementLevel{D, [Tv]}()\n\nA subfiber of an element level is a scalar of type Tv, initialized to D. D may optionally be given as the first argument.\n\nIn the @fiber constructor, e is an alias for ElementLevel.\n\njulia> @fiber(d(e(0.0)), [1, 2, 3])\nDense [1:3]\n├─[1]: 1.0\n├─[2]: 2.0\n├─[3]: 3.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseListLevel","page":"Array Formats","title":"Finch.SparseListLevel","text":"SparseListLevel{[Ti=Int], [Tp=Int]}(lvl, [dim])\n\nA subfiber of a sparse level does not need to represent slices A[:, ..., :, i] which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl.  A sorted list is used to record which slices are stored. Optionally, dim is the size of the last dimension.\n\nTi is the type of the last fiber index, and Tp is the type used for positions in the level.\n\nIn the @fiber constructor, sl is an alias for SparseListLevel.\n\njulia> @fiber(d(sl(e(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseList (0.0) [1:3]\n├─[:,3]: SparseList (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> @fiber(sl(sl(e(0.0))), [10 0 20; 30 0 0; 0 0 40])\nSparseList (0.0) [:,1:3]\n├─[:,1]: SparseList (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,3]: SparseList (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseCOOLevel","page":"Array Formats","title":"Finch.SparseCOOLevel","text":"SparseCOOLevel{[N], [Ti=Tuple{Int...}], [Tp=Int]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl. The sparse coo level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A set of N lists (one for each index) are used to record which slices are stored. The coordinates (sets of N indices) are sorted in column major order.  Optionally, dims are the sizes of the last dimensions.\n\nTi is the type of the last N fiber indices, and Tp is the type used for positions in the level.\n\nIn the @fiber constructor, sh is an alias for SparseCOOLevel.\n\njulia> @fiber(d(sc{1}(e(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseCOO (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseCOO (0.0) [1:3]\n├─[:,3]: SparseCOO (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> @fiber(sc{2}(e(0.0)), [10 0 20; 30 0 0; 0 0 40])\nSparseCOO (0.0) [1:3,1:3]\n├─├─[1, 1]: 10.0\n├─├─[2, 1]: 30.0\n├─├─[1, 3]: 20.0\n├─├─[3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"fibers/#Finch.SparseHashLevel","page":"Array Formats","title":"Finch.SparseHashLevel","text":"SparseHashLevel{[N], [Ti=Tuple{Int...}], [Tp=Int]}(lvl, [dims])\n\nA subfiber of a sparse level does not need to represent slices which are entirely default. Instead, only potentially non-default slices are stored as subfibers in lvl. The sparse hash level corresponds to N indices in the subfiber, so fibers in the sublevel are the slices A[:, ..., :, i_1, ..., i_n].  A hash table is used to record which slices are stored. Optionally, dims are the sizes of the last dimensions.\n\nTi is the type of the last N fiber indices, and Tp is the type used for positions in the level.\n\nIn the @fiber constructor, sh is an alias for SparseHashLevel.\n\njulia> @fiber(d(sh{1}(e(0.0))), [10 0 20; 30 0 0; 0 0 40])\nDense [:,1:3]\n├─[:,1]: SparseHash (0.0) [1:3]\n│ ├─[1]: 10.0\n│ ├─[2]: 30.0\n├─[:,2]: SparseHash (0.0) [1:3]\n├─[:,3]: SparseHash (0.0) [1:3]\n│ ├─[1]: 20.0\n│ ├─[3]: 40.0\n\njulia> @fiber(sh{2}(e(0.0)), [10 0 20; 30 0 0; 0 0 40])\nSparseHash (0.0) [1:3,1:3]\n├─├─[1, 1]: 10.0\n├─├─[2, 1]: 30.0\n├─├─[1, 3]: 20.0\n├─├─[3, 3]: 40.0\n\n\n\n\n\n","category":"type"},{"location":"directory_structure/#Directory-Structure","page":"Directory Structure","title":"Directory Structure","text":"","category":"section"},{"location":"directory_structure/","page":"Directory Structure","title":"Directory Structure","text":"Here's a little roadmap to the Finch codebase! Please file an issue if this is not up to date.","category":"page"},{"location":"directory_structure/","page":"Directory Structure","title":"Directory Structure","text":".\n├── apps                    # Example applications implemented in Finch!\n│   ├── graphs.jl           # Graph Algorithms: Pagerank, Bellman-Ford, etc...\n│   ├── linalg.jl           # Linear Algebra: Sparse-Sparse Matmul, etc...\n│   └── ...\n├── benchmark               # benchmarks for internal use\n│   ├── runbenchmarks.jl    # run benchmarks\n│   ├── runjudge.jl         # run benchmarks on current branch and compare with main\n│   └── ...                 \n├── docs                    # documentation\n│   ├── [build]             # rendered docs website\n│   ├── src                 # docs website source\n│   ├── fix.jl              # fix docstrings\n│   ├── make.jl             # deploys documentation locally\n│   └── ...                 \n├── embed                   # wrappers for embedding Finch in C\n├── ext                     # conditionally-loaded code for interaction with other packages (e.g. SparseArrays)\n├── src                     # Source files\n│   ├── base                # Implementations of base functions (e.g. map, reduce, etc.)\n│   ├── fileio              # File IO function definitions\n│   ├── FinchNotation       # SubModule containing the Finch IR\n│   │   ├── nodes.jl        # defines the Finch IR\n│   │   ├── syntax.jl       # defines the @finch frontend syntax\n│   │   └── ...\n│   ├── looplets            # this is where all the Looplets live\n│   ├── symbolic            # term rewriting systems for program and bounds\n│   ├── tensors             # built-in Finch tensor definitions\n│   │   ├── levels          # all of the levels\n│   │   ├── fibers.jl       # fibers combine levels to form tensors\n│   │   ├── scalars.jl      # a nice scalar type\n│   │   ├── masks.jl        # mask tensors (e.g. upper-triangular mask)\n│   │   └── modifiers.jl    # index modifiers (e.g. A[i + j])\n│   ├──  execute.jl         # toplevel compiler calls\n│   ├──  semantics.jl       # finch array interface functions\n│   ├──  traits.jl          # functions and types to reason about appropriate outputs\n│   ├──  util.jl            # shims and julia codegen utils (Dead code elimination, etc...)\n│   └── ...\n├── test                    # tests\n│   ├──  embed              # tests for the C embedding. Optional build before runtests.jl\n│   ├──  reference32        # reference output for 32-bit systems\n│   ├──  reference64        # reference output for 64-bit systems\n│   ├──  runtests.jl        # runs the test suite. (pass -h for options and more info!)\n│   └── ...\n├── Project.toml            # julia-readable listing of project dependencies\n├── [Manifest.toml]         # local listing of installed dependencies (don't commit this)\n├── LICENSE\n└── README.md","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"We welcome contributions to Finch, and follow the Julia contributing guidelines.  If you use or want to use Finch and have a question or bug, please do file a Github issue!  If you want to contribute to Finch, please first file an issue to double check that there is interest from a contributor in the feature.","category":"page"},{"location":"CONTRIBUTING/#Testing","page":"Contributing","title":"Testing","text":"","category":"section"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"All pull requests should pass continuous integration testing before merging. The test suite has a few options, which are accessible through running the test suite directly as julia tests/runtests.jl.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"Finch compares compiler output against reference versions. If you run the test suite (test/runtests.jl) directly you can pass the --overwrite flag to tell the test suite to overwrite the reference.  Because the reference output depends on the system word size, you'll need to generate reference output for 32-bit and 64-bit builds of Julia to get Finch to pass tests. The easiest way to do this is to run each 32-bit or 64-bit build of Julia on a system that supports it. You can Download multiple builds yourself or use juliaup to manage multiple versions. Using juliaup, it might look like this:","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"julia +release~x86 tests/runtests.jl --overwrite\njulia +release~x64 tests/runtests.jl --overwrite","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"The test suite takes a while to run. You can filter to only run a selection of test suites by specifying them as positional arguments, e.g.","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"julia tests/runtests.jl constructors conversions representation","category":"page"},{"location":"CONTRIBUTING/","page":"Contributing","title":"Contributing","text":"This information is summarized with julia tests/runtests.jl --help","category":"page"},{"location":"README/#Finch.jl","page":"Finch.jl","title":"Finch.jl","text":"","category":"section"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"[docs]:https://willow-ahrens.github.io/Finch.jl/stable [ddocs]:https://willow-ahrens.github.io/Finch.jl/dev [ci]:https://github.com/willow-ahrens/Finch.jl/actions/workflows/CI.yml?query=branch%3Amain [cov]:https://codecov.io/gh/willow-ahrens/Finch.jl [tool]:https://mybinder.org/v2/gh/willow-ahrens/Finch.jl/gh-pages?labpath=dev%2Finteractive.ipynb","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"[docsico]:https://img.shields.io/badge/docs-stable-blue.svg [ddocsico]:https://img.shields.io/badge/docs-dev-blue.svg [ciico]:https://github.com/willow-ahrens/Finch.jl/actions/workflows/CI.yml/badge.svg?branch=main [covico]:https://codecov.io/gh/willow-ahrens/Finch.jl/branch/main/graph/badge.svg [toolico]:https://mybinder.org/badgelogo.svg","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Documentation Build Status Try It Online!\n[![][docsico]][docs] [![][ddocsico]][ddocs] [![][ciico]][ci] [![][covico]][cov] [![][tool_ico]][tool]","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Finch is a adaptable Julia-to-Julia compiler for loop nests over sparse or structured multidimensional arrays. In addition to supporting sparse arrays, Finch can also handle custom operators and fill values other than zero, runs of repeated values, or even special structures such as clustered nonzeros or triangular patterns.","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Finch allows you to write for-loops as if they are dense, but compile them to be sparse! The compiler takes care of applying rules like x * 0 => 0 and the like to avoid redundant computation.  Finch also supports if-statements and custom user types and functions.  Users can add rewrite rules to inform the compiler about any special user-defined properties or optimizations.  You can even modify indexing expressions to express sparse convolution, or to describe windows into structured arrays.","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"As an example, here's a program which calculates the minimum, maximum, sum, and variance of a sparse vector, reading the vector only once, and only reading nonzero values:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"using Finch\n\nX = @fiber(sl(e(0.0)), fsprand((10,), 0.5))\nx = Scalar(0.0)\nx_min = Scalar(Inf)\nx_max = Scalar(-Inf)\nx_sum = Scalar(0.0)\nx_var = Scalar(0.0)\n@finch begin\n    for i = _\n        x .= 0\n        x[] = X[i]\n        x_min[] <<min>>= x[]\n        x_max[] <<max>>= x[]\n        x_sum[] += x[]\n        x_var[] += x[] * x[]\n    end\nend;","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"Array formats in Finch are described recursively mode by mode.  Semantically, an array in Finch can be understood as a tree, where each level in the tree corresponds to a dimension and each edge corresponds to an index. For example, @fiber(d(sl(e(0.0)))) constructs a Float64 CSC-format sparse matrix, and @fiber(sl(sl(e(0.0)))) constructs a DCSC-format hypersparse matrix. As another example, here's a column-major sparse matrix-vector multiply:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"x = @fiber(d(e(0.0)), rand(42));\nA = @fiber(d(sl(e(0.0))), fsprand((42, 42), 0.1));\ny = @fiber(d(e(0.0)));\n@finch begin\n    y .= 0\n    for j=_, i=_\n        y[i] += A[i, j] * x[j]\n    end\nend;","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"At it's heart, Finch is powered by a new domain specific language for coiteration, breaking structured iterators into control flow units we call Looplets. Looplets are lowered progressively with several stages for rewriting and simplification.","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"The technologies enabling Finch are described in our manuscript.","category":"page"},{"location":"README/#Installation","page":"Finch.jl","title":"Installation","text":"","category":"section"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"At the Julia REPL, install the latest stable version by running:","category":"page"},{"location":"README/","page":"Finch.jl","title":"Finch.jl","text":"julia> using Pkg; Pkg.add(\"Finch\")","category":"page"},{"location":"interop/#Using-Finch-with-Other-Languages","page":"C, C++, ...","title":"Using Finch with Other Languages","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"You can use Finch in other languages through our C interface! We also include convenience types for converting between 0-indexed and 1-indexed arrays.","category":"page"},{"location":"interop/#finch.h","page":"C, C++, ...","title":"finch.h","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"Refer to finch.h for detailed documentation. The public functions include a few shortcuts for constructing finch datatypes, as well as convenience functions for calling Julia from C. Refer also to the Julia documentation for more general advice. Refer to the tests for a working example of embedding in C. Note that calling finch_init will call jl_init, as well as initializing a few function pointers for the interface. Julia cannot see C references to Julia objects, so finch.h includes a few functions to introduce references on the Julia side that mirror C objects.","category":"page"},{"location":"interop/#Index-Compatibility","page":"C, C++, ...","title":"0-Index Compatibility","text":"","category":"section"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"Julia, Matlab, etc. index arrays starting at 1. C, python, etc. index starting at 0. In a dense array, we can simply subtract one from the index, and in fact, this is what Julia will does under the hood when you pass a vector between C to Julia. ","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"However, for sparse array formats, it's not just a matter of subtracting one from the index, as the internal lists of indices, positions, etc all start from zero as well. To remedy the situation, Finch interoperates with the CIndices package, which exports a  type called CIndex. The internal representation of CIndex is one less than the value it represents, and we can use CIndex as the index or position type of a Finch array to represent arrays in other languages.","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"For example, if idx_c, ptr_c, and val_c are the internal arrays of a CSC matrix in a zero-indexed language, we can represent that matrix as a one-indexed Finch array without copying by calling","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"DocTestSetup = quote\n    using Finch\n    using CIndices\nend","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"julia> m = 4; n = 3; ptr_c = [0, 3, 3, 5]; idx_c = [1, 2, 3, 0, 2]; val_c = [1.1, 2.2, 3.3, 4.4, 5.5];\n\njulia> ptr_jl = unsafe_wrap(Array, reinterpret(Ptr{CIndex{Int}}, pointer(ptr_c)), length(ptr_c); own = false)\n4-element Vector{CIndex{Int64}}:\n CIndex{Int64}(0)\n CIndex{Int64}(3)\n CIndex{Int64}(3)\n CIndex{Int64}(5)\njulia> idx_jl = unsafe_wrap(Array, reinterpret(Ptr{CIndex{Int}}, pointer(idx_c)), length(idx_c); own = false)\n5-element Vector{CIndex{Int64}}:\n CIndex{Int64}(1)\n CIndex{Int64}(2)\n CIndex{Int64}(3)\n CIndex{Int64}(0)\n CIndex{Int64}(2)\njulia> A = Fiber(Dense(SparseList{CIndex{Int}, CIndex{Int}}(Element{0.0, Float64}(val_c), m, ptr_jl, idx_jl), n))\nDense [:,1:3]\n├─[:,1]: SparseList (0.0) [1:CIndex{Int64}(3)]\n│ ├─[CIndex{Int64}(1)]: 1.1\n│ ├─[CIndex{Int64}(2)]: 2.2\n│ ├─[CIndex{Int64}(3)]: 3.3\n├─[:,2]: SparseList (0.0) [1:CIndex{Int64}(3)]\n├─[:,3]: SparseList (0.0) [1:CIndex{Int64}(3)]\n│ ├─[CIndex{Int64}(0)]: 4.4\n│ ├─[CIndex{Int64}(2)]: 5.5","category":"page"},{"location":"interop/","page":"C, C++, ...","title":"C, C++, ...","text":"We can also convert between representations by by copying to or from CIndex fibers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Finch","category":"page"},{"location":"#Finch","page":"Home","title":"Finch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Finch is an adaptable compiler for loop nests over sparse or otherwise structured arrays. Finch supports general sparsity as well as many specialized sparsity patterns, like clustered nonzeros, diagonals, or triangles.  In addition to zero, Finch supports optimizations over arbitrary fill values and operators, even run-length-compression.","category":"page"},{"location":"","page":"Home","title":"Home","text":"At it's heart, Finch is powered by a domain specific language for coiteration, breaking structured iterators into units we call Looplets. The Looplets are lowered progressively, leaving several opportunities to rewrite and simplify intermediate expressions.","category":"page"},{"location":"#Installation:","page":"Home","title":"Installation:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"Finch\")","category":"page"},{"location":"#Usage:","page":"Home","title":"Usage:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We're working on adding more documentation, for now take a look at the examples for linear algebra or graphs.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"CurrentModule = Finch","category":"page"},{"location":"algebra/#Custom-Functions","page":"Custom Functions","title":"Custom Functions","text":"","category":"section"},{"location":"algebra/#User-Functions","page":"Custom Functions","title":"User Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch supports arbitrary Julia Base functions over isbits types.  You can also use your own functions and use them in Finch! Just remember to define any special algebraic properties of your functions so that Finch can optimize them better. You must declare the properties of your functions before you call any Finch functions on them.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch only supports incrementing assignments to arrays such as += or *=. If you would like to increment A[i...] by the value of ex with a custom reduction operator op, you may use the following syntax: A[i...] <<op>>= ex.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Consider the greatest common divisor function gcd. This function is associative and commutative, and the greatest common divisor of 1 and anything else is 1, so 1 is an annihilator.  We declare these properties by overloading trait functions on Finch's default algebra as follows:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch.isassociative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::Finch.DefaultAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::Finch.DefaultAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Then, the following code will only call gcd when neither u[i] nor v[i] are 1 (just once!).","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"u = @fiber(sl(e(1)), [3, 1, 6, 1, 9, 1, 4, 1, 8, 1])\nv = @fiber(sl(e(1)), [1, 2, 3, 1, 1, 1, 1, 4, 1, 1])\nw = @fiber sl(e(1))\n\n@finch MyAlgebra() (w .= 1; @loop i w[i] = gcd(u[i], v[i]))","category":"page"},{"location":"algebra/#A-Few-Convenient-Functions","page":"Custom Functions","title":"A Few Convenient Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"For your convenience, Finch defines a few useful functions that help express common array operations inside Finch:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"choose\nminby\nmaxby","category":"page"},{"location":"algebra/#Finch.choose","page":"Custom Functions","title":"Finch.choose","text":"choose(z)(a, b)\n\nchoose(z) is a function which returns whichever of a or b is not isequal to z. If neither are z, then return a. Useful for getting the first nonfill value in a sparse array.\n\njulia> a = @fiber(sl(e(0.0)), [0, 1.1, 0, 4.4, 0])\nSparseList (0.0) [1:5]\n├─[2]: 1.1\n├─[4]: 4.4\n\njulia> x = Scalar(0.0); @finch @loop i x[] <<choose(0.0)>>= a[i];\n\njulia> x[]\n1.1\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.minby","page":"Custom Functions","title":"Finch.minby","text":"minby(a, b)\n\nReturn the min of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmin operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(Inf => 0);\n\njulia> @finch @loop i x[] <<minby>>= a[i] => i;\n\njulia> x[]\n3.3 => 2\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.maxby","page":"Custom Functions","title":"Finch.maxby","text":"maxby(a, b)\n\nReturn the max of a or b, comparing them by a[1] and b[1], and breaking ties to the left. Useful for implementing argmax operations:\n\njulia> a = [7.7, 3.3, 9.9, 3.3, 9.9]; x = Scalar(-Inf => 0);\n\njulia> @finch @loop i x[] <<maxby>>= a[i] => i;\n\njulia> x[]\n9.9 => 3\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Properties","page":"Custom Functions","title":"Properties","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"The full list of properties recognized by Finch is as follows (use these to declare the properties of your own functions):","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"isassociative\niscommutative\nisdistributive\nisidempotent\nisidentity\nisannihilator\nisinverse\nisinvolution","category":"page"},{"location":"algebra/#Finch.isassociative","page":"Custom Functions","title":"Finch.isassociative","text":"isassociative(algebra, f)\n\nReturn true when f(a..., f(b...), c...) = f(a..., b..., c...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.iscommutative","page":"Custom Functions","title":"Finch.iscommutative","text":"iscommutative(algebra, f)\n\nReturn true when for all permutations p, f(a...) = f(a[p]...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isdistributive","page":"Custom Functions","title":"Finch.isdistributive","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isidempotent","page":"Custom Functions","title":"Finch.isidempotent","text":"isidempotent(algebra, f)\n\nReturn true when f(a, b) = f(f(a, b), b) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isidentity","page":"Custom Functions","title":"Finch.isidentity","text":"isidentity(algebra, f, x)\n\nReturn true when f(a..., x, b...) = f(a..., b...) in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isannihilator","page":"Custom Functions","title":"Finch.isannihilator","text":"isannihilator(algebra, f, x)\n\nReturn true when f(a..., x, b...) = x in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isinverse","page":"Custom Functions","title":"Finch.isinverse","text":"isinverse(algebra, f, g)\n\nReturn true when f(a, g(a)) is the identity under f in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.isinvolution","page":"Custom Functions","title":"Finch.isinvolution","text":"isinvolution(algebra, f)\n\nReturn true when f(f(a)) = a in algebra.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch-Kernel-Caching","page":"Custom Functions","title":"Finch Kernel Caching","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch code is cached when you first run it. Thus, if you run a Finch function once, then make changes to the Finch compiler (like defining new properties), the cached code will be used and the changes will not be reflected.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"It's best to design your code so that modifications to the Finch compiler occur before any Finch functions are called. However, if you really need to modify a precompiled Finch kernel, you can call Finch.refresh() to invalidate the code cache.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"refresh","category":"page"},{"location":"algebra/#Finch.refresh","page":"Custom Functions","title":"Finch.refresh","text":"Finch.refresh()\n\nFinch caches the code for kernels as soon as they are run. If you modify the Finch compiler after running a kernel, you'll need to invalidate the Finch caches to reflect these changes by calling Finch.refresh(). This function should only be called at global scope, and never during precompilation.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#(Advanced)-On-World-Age-and-Generated-Functions","page":"Custom Functions","title":"(Advanced) On World-Age and Generated Functions","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Julia uses a \"world age\" to describe the set of defined functions at a point in time. Generated functions run in the same world age in which they were defined, so they can't call functions defined after the generated function. This means that if Finch used normal generated functions, users can't define their own functions without first redefining all of Finch's generated functions.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Finch uses special generators that run in the current world age, but do not update with subsequent compiler function invalidations. If two packages modify the behavior of Finch in different ways, and call those Finch functions during precompilation, the resulting behavior is undefined.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"There are several packages that take similar, but different, approaches to allow user participation in staged Julia programming (not to mention Base eval or @generated): StagedFunctions.jl, GeneralizedGenerated.jl, RuntimeGeneratedFunctions.jl, or Zygote.jl.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Our approach is most similar to that of StagedFunctions.jl or Zygote.jl. We chose our approach to be the simple and flexible while keeping the kernel call overhead low.","category":"page"},{"location":"algebra/#(Advanced)-Separate-Algebras","page":"Custom Functions","title":"(Advanced) Separate Algebras","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"If you want to define non-standard properties or custom rewrite rules for some functions in a separate context, you can represent these changes with your own algebra type.  We express this by subtyping AbstractAlgebra and defining properties as follows:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"struct MyAlgebra <: AbstractAlgebra end\n\nFinch.isassociative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.iscommutative(::MyAlgebra, ::typeof(gcd)) = true\nFinch.isannihilator(::MyAlgebra, ::typeof(gcd), x) = x == 1","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"We pass the algebra to Finch as an optional first argument:","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"@finch MyAlgebra() (w .= 1; @loop i w[i] = gcd(u[i], v[i]))","category":"page"},{"location":"algebra/#Rewriting","page":"Custom Functions","title":"Rewriting","text":"","category":"section"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"Define custom rewrite rules by overloading the get_program_rules function on your algebra.  Unless you want to write the full rule set from scratch, be sure to append your new rules to the old rules, which can be obtained by calling get_program_rules with another algebra. Rules can be specified directly on Finch IR using RewriteTools.jl.","category":"page"},{"location":"algebra/","page":"Custom Functions","title":"Custom Functions","text":"get_program_rules\nget_bounds_rules","category":"page"},{"location":"algebra/#Finch.get_program_rules","page":"Custom Functions","title":"Finch.get_program_rules","text":"get_program_rules(alg, shash)\n\nReturn the program rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. Defaults to a collection of straightforward rules that use the algebra to check properties of functions like associativity, commutativity, etc. shash is an object that can be called to return a static hash value. This rule set simplifies, normalizes, and propagates constants, and is the basis for how Finch understands sparsity.\n\n\n\n\n\n","category":"function"},{"location":"algebra/#Finch.get_bounds_rules","page":"Custom Functions","title":"Finch.get_bounds_rules","text":"get_bounds_rules(alg, shash)\n\nReturn the bound rule set for Finch. One can dispatch on the alg trait to specialize the rule set for different algebras. shash is an object that can be called to return a static hash value. This rule set is used to analyze loop bounds in Finch.\n\n\n\n\n\n","category":"function"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"CurrentModule = Finch","category":"page"},{"location":"fileio/#Finch-Tensor-File-Input/Output","page":"Tensor File I/O","title":"Finch Tensor File Input/Output","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports many input/output tensor file formats.","category":"page"},{"location":"fileio/#Finch-Format-(.fbr)","page":"Tensor File I/O","title":"Finch Format (.fbr)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch's custom binary file format for fibers is best suited to users who plan to exclusively use Finch in Julia (perhaps across different platforms). This format straightforwardly maps the fields of Finch fiber formats to arrays in data containers (currently Finch only supports HDF5, but if you file an issue someone might add Numpy NPZ). Arrays are stored 1-indexed as they would be in memory.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"fbrwrite\nfbrread","category":"page"},{"location":"fileio/#Finch.fbrwrite","page":"Tensor File I/O","title":"Finch.fbrwrite","text":"fbrwrite(filename, tns)\n\nWrite the Finch fiber using Finch's custom binary HDF5 file format.\n\nHDF5 must be loaded for this function to be available\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.fbrread","page":"Tensor File I/O","title":"Finch.fbrread","text":"fbrread(filename)\n\nRead the Finch fiber using Finch's custom binary HDF5 file format.\n\nHDF5 must be loaded for this function to be available\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Binsparse-Format-(.fbr)","page":"Tensor File I/O","title":"Binsparse Format (.fbr)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports the most recent revision of the Binsparse binary sparse tensor format, as well as the proposed v2.0 tensor extension. This is a good option for those who want an efficient way to transfer sparse tensors between supporting libraries and languages. The Binsparse format represents the tensor format as a JSON string in the underlying data container (currently Finch only supports HDF5, but if you file an issue someone might add Numpy NPZ). Binsparse arrays are stored 0-indexed.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"bswrite\nbsread","category":"page"},{"location":"fileio/#Finch.bswrite","page":"Tensor File I/O","title":"Finch.bswrite","text":"bswrite(filename, tns)\n\nWrite the Finch fiber to a file using  Binsparse HDF5 file format.\n\nHDF5 must be loaded for this function to be available\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.bsread","page":"Tensor File I/O","title":"Finch.bsread","text":"bsread(filename)\n\nRead the Binsparse HDF5 file into a Finch tensor.\n\nHDF5 must be loaded for this function to be available\n\nwarning: Warning\nThe Binsparse spec is under development. Additionally, this function may not be fully conformant. Please file bug reports if you see anything amiss.\n\n\n\n\n\n","category":"function"},{"location":"fileio/#TensorMarket-(.mtx,-.ttx)","page":"Tensor File I/O","title":"TensorMarket (.mtx, .ttx)","text":"","category":"section"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"Finch supports the MatrixMarket and TensorMarket formats, which prioritize readability and archiveability, storing matrices and tensors in plaintext.","category":"page"},{"location":"fileio/","page":"Tensor File I/O","title":"Tensor File I/O","text":"fttwrite\nfttread","category":"page"},{"location":"fileio/#Finch.fttwrite","page":"Tensor File I/O","title":"Finch.fttwrite","text":"fttwrite(filename, tns)\n\nWrite a sparse Finch fiber to a TensorMarket file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttwrite\n\n\n\n\n\n","category":"function"},{"location":"fileio/#Finch.fttread","page":"Tensor File I/O","title":"Finch.fttread","text":"fttread(filename, infoonly=false, retcoord=false)\n\nRead the TensorMarket file into a Finch fiber. The fiber will be dense or COO depending on the format of the file.\n\nTensorMarket must be loaded for this function to be available.\n\nSee also: ttread\n\n\n\n\n\n","category":"function"}]
}
